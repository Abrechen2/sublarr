---
phase: 02-translation-multi-backend
plan: 03
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - backend/translation/openai_compat.py
  - backend/translation/google_translate.py
  - backend/translation/__init__.py
  - backend/requirements.txt
autonomous: true

must_haves:
  truths:
    - "OpenAI-compatible backend translates text using chat completions API with configurable base_url"
    - "OpenAI-compatible backend reuses shared LLM utilities for prompt building and response parsing"
    - "Google Cloud Translation backend translates text using official v3 SDK"
    - "Both backends implement health_check that verifies service availability"
    - "Both backends are registered in TranslationManager"
  artifacts:
    - path: "backend/translation/openai_compat.py"
      provides: "OpenAI-compatible backend (OpenAI, Azure, LM Studio, vLLM)"
      contains: "class OpenAICompatBackend"
    - path: "backend/translation/google_translate.py"
      provides: "Google Cloud Translation v3 backend"
      contains: "class GoogleTranslateBackend"
  key_links:
    - from: "backend/translation/openai_compat.py"
      to: "backend/translation/llm_utils.py"
      via: "shared prompt building and response parsing"
      pattern: "from translation\\.llm_utils import"
    - from: "backend/translation/openai_compat.py"
      to: "backend/translation/base.py"
      via: "ABC inheritance"
      pattern: "class OpenAICompatBackend.*TranslationBackend"
    - from: "backend/translation/google_translate.py"
      to: "backend/translation/base.py"
      via: "ABC inheritance"
      pattern: "class GoogleTranslateBackend.*TranslationBackend"
---

<objective>
Implement the OpenAI-compatible LLM backend and Google Cloud Translation API backend, completing the set of 5 translation backends. The OpenAI backend shares LLM utilities with Ollama for prompt building and response parsing.

Purpose: OpenAI-compatible backend covers OpenAI, Azure OpenAI, LM Studio, and vLLM via a single implementation using the `base_url` parameter. Google backend provides enterprise-grade API translation with glossary support.
Output: Two working backend implementations registered in TranslationManager.
</objective>

<execution_context>
@C:/Users/Dennis Wittke/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Dennis Wittke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-translation-multi-backend/02-RESEARCH.md
@.planning/phases/02-translation-multi-backend/02-01-SUMMARY.md

@backend/translation/base.py
@backend/translation/llm_utils.py
@backend/translation/__init__.py
@backend/requirements.txt
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement OpenAI-compatible translation backend</name>
  <files>
    backend/translation/openai_compat.py
    backend/requirements.txt
  </files>
  <action>
**requirements.txt** — Add `openai>=1.0.0` to the dependencies.

**openai_compat.py** — OpenAI-compatible backend implementing TranslationBackend:
- Class attributes: `name = "openai_compat"`, `display_name = "OpenAI-Compatible (OpenAI, Azure, LM Studio, vLLM)"`, `supports_glossary = True` (via prompt injection), `supports_batch = True`, `max_batch_size = 25`
- `config_fields`: api_key (password, required), base_url (text, required, default "https://api.openai.com/v1"), model (text, required, default "gpt-4o-mini"), temperature (number, optional, default "0.3"), request_timeout (number, optional, default "120"), max_retries (number, optional, default "3")
- Lazy client: `_get_client()` creates `OpenAI(api_key=..., base_url=..., timeout=..., max_retries=...)`, caches in `self._client`
- `translate_batch()`:
  1. Get client
  2. Build prompt using `from translation.llm_utils import build_translation_prompt` — same function Ollama uses
  3. Call `client.chat.completions.create(model=..., messages=[{"role": "user", "content": prompt}], temperature=...)` — NOT streaming
  4. Extract response text: `completion.choices[0].message.content.strip()`
  5. Parse with `from translation.llm_utils import parse_llm_response` — same parser Ollama uses
  6. Check CJK hallucination with `from translation.llm_utils import has_cjk_hallucination`
  7. If parse fails (None) or CJK hallucination detected: retry up to max_retries times with backoff
  8. Return TranslationResult with translated_lines, characters_used
  9. On final failure: return TranslationResult with success=False
- `health_check()`:
  1. Call `client.models.list()`, extract first 10 model IDs
  2. Check if configured model is in the list
  3. Return `(True, f"OK (model '{model}' available)")` or `(False, f"Model '{model}' not found. Available: {model_names}")`
  4. Catch exceptions, return `(False, str(e))`
- `get_config_fields()`: Return `self.config_fields`
- Import guard: Wrap `from openai import OpenAI` in try/except ImportError, log warning, make translate_batch raise RuntimeError("openai package not installed")
  </action>
  <verify>
Run: `cd backend && python -c "from translation.openai_compat import OpenAICompatBackend; b = OpenAICompatBackend(); assert b.name == 'openai_compat'; assert b.supports_glossary == True; assert b.max_batch_size == 25; print('OpenAI-compat backend OK')"`
  </verify>
  <done>
OpenAICompatBackend implements TranslationBackend ABC. Shares LLM utilities with OllamaBackend for prompt building and response parsing. Configurable base_url supports OpenAI, Azure, LM Studio, and vLLM endpoints.
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement Google Cloud Translation backend and register both backends</name>
  <files>
    backend/translation/google_translate.py
    backend/translation/__init__.py
    backend/requirements.txt
  </files>
  <action>
**requirements.txt** — Add `google-cloud-translate>=3.10.0` to the dependencies.

**google_translate.py** — Google Cloud Translation v3 backend implementing TranslationBackend:
- Class attributes: `name = "google"`, `display_name = "Google Cloud Translation"`, `supports_glossary = True` (native API support), `supports_batch = True`, `max_batch_size = 1024`
- `config_fields`: project_id (text, required), credentials_path (text, optional, help "Path to service account JSON or set GOOGLE_APPLICATION_CREDENTIALS env var"), location (text, optional, default "global")
- `translate_batch()`:
  1. Create `translate_v3.TranslationServiceClient()` — SDK reads credentials from env or credentials_path config
  2. Build parent string: `f"projects/{project_id}/locations/{location}"`
  3. If credentials_path is set and non-empty, set `os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = credentials_path` before creating client
  4. Call `client.translate_text(request={"parent": parent, "contents": lines, "mime_type": "text/plain", "source_language_code": source_lang, "target_language_code": target_lang})`
  5. Extract: `[t.translated_text for t in response.translations]`
  6. Return TranslationResult with translated_lines, characters_used
  7. On exception: return TranslationResult with success=False
- `health_check()`:
  1. Create client, build parent string
  2. Call `client.get_supported_languages(request={"parent": parent})`
  3. Return `(True, f"OK ({len(response.languages)} languages)")`
  4. Catch exceptions (especially `DefaultCredentialsError`), return `(False, str(e))`
- `get_config_fields()`: Return `self.config_fields`
- `get_usage()`: Return empty dict (Google uses billing, no per-request quota in API response)
- Import guard: Wrap `from google.cloud import translate_v3` in try/except ImportError, log warning, raise RuntimeError if used without package

**__init__.py** — Register both new backends at bottom of file:
- Import OpenAICompatBackend and GoogleTranslateBackend with try/except ImportError guards
- Register each via `register_backend()`
- After this, all 5 backends should be registered: ollama, deepl, libretranslate, openai_compat, google
  </action>
  <verify>
Run: `cd backend && python -c "
from translation import get_translation_manager
m = get_translation_manager()
backends = sorted(m._backend_classes.keys())
print('All backends:', backends)
assert 'ollama' in backends
assert 'libretranslate' in backends
# openai_compat and google may not be available without pip install
print('Total registered:', len(backends))
print('Registration OK')
"`
  </verify>
  <done>
GoogleTranslateBackend implements TranslationBackend ABC with v3 SDK. All 5 backends are registered in TranslationManager. Optional dependencies (deepl, openai, google-cloud-translate) degrade gracefully if not installed. requirements.txt lists all dependencies.
  </done>
</task>

</tasks>

<verification>
1. `from translation.openai_compat import OpenAICompatBackend` — imports and instantiates
2. `from translation.google_translate import GoogleTranslateBackend` — imports and instantiates
3. OpenAI backend shares `build_translation_prompt` and `parse_llm_response` with Ollama backend
4. TranslationManager registers all available backends (at least ollama + libretranslate always available)
5. `openai>=1.0.0` and `google-cloud-translate>=3.10.0` appear in requirements.txt
</verification>

<success_criteria>
- OpenAI-compatible backend supports any OpenAI-compatible endpoint via base_url
- Google Cloud Translation backend uses v3 SDK with credentials management
- Both backends have working health_check methods
- LLM backends (Ollama + OpenAI) share prompt/parse utilities; API backends (DeepL, LibreTranslate, Google) do not use them
- All 5 backends registered; missing optional packages do not break app startup
</success_criteria>

<output>
After completion, create `.planning/phases/02-translation-multi-backend/02-03-SUMMARY.md`
</output>
