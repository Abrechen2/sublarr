---
phase: 15-api-key-mgmt-notifications-cleanup
plan: 03
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/db/models/cleanup.py
  - backend/db/models/__init__.py
  - backend/db/repositories/cleanup.py
  - backend/db/repositories/__init__.py
  - backend/dedup_engine.py
  - backend/cleanup_scheduler.py
  - backend/routes/cleanup.py
  - backend/routes/__init__.py
autonomous: true

must_haves:
  truths:
    - "Deduplication engine scans media paths and identifies duplicate subtitles by SHA-256 content hash"
    - "Duplicate groups show file paths, sizes, formats, and languages for informed deletion decisions"
    - "Batch delete enforces keep-at-least-one guard per duplicate group"
    - "Cleanup rules are configurable and can run on a schedule"
    - "Disk space analysis shows total subtitle storage, duplicate waste, and trends over time"
    - "Background scan runs in ThreadPoolExecutor with WebSocket progress updates"
  artifacts:
    - path: "backend/db/models/cleanup.py"
      provides: "SubtitleHash, CleanupRule, CleanupHistory ORM models"
      exports: ["SubtitleHash", "CleanupRule", "CleanupHistory"]
    - path: "backend/dedup_engine.py"
      provides: "Content hashing, duplicate detection, batch delete with safety guard"
      exports: ["compute_subtitle_hash", "scan_for_duplicates", "delete_duplicates"]
    - path: "backend/cleanup_scheduler.py"
      provides: "Scheduled cleanup runner following existing scheduler patterns"
      exports: ["start_cleanup_scheduler"]
    - path: "backend/routes/cleanup.py"
      provides: "Blueprint for /api/v1/cleanup/* endpoints"
      exports: ["bp"]
  key_links:
    - from: "backend/dedup_engine.py"
      to: "backend/db/repositories/cleanup.py"
      via: "store/query subtitle hashes and cleanup history"
      pattern: "CleanupRepository"
    - from: "backend/routes/cleanup.py"
      to: "backend/dedup_engine.py"
      via: "scan and delete operations triggered by API"
      pattern: "scan_for_duplicates|delete_duplicates"
    - from: "backend/cleanup_scheduler.py"
      to: "backend/dedup_engine.py"
      via: "scheduled scan execution"
      pattern: "scan_for_duplicates"
---

<objective>
Deduplication engine, cleanup system, and disk space analysis backend.

Purpose: Provides content-hash based subtitle deduplication, configurable cleanup rules with scheduling, and disk space analysis. Background scanning uses ThreadPoolExecutor with WebSocket progress updates.
Output: Cleanup models + repository + dedup engine + scheduler + API Blueprint
</objective>

<execution_context>
@C:/Users/Dennis Wittke/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Dennis Wittke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/15-api-key-mgmt-notifications-cleanup/15-RESEARCH.md

@backend/db/models/__init__.py
@backend/db/repositories/__init__.py
@backend/routes/__init__.py
@backend/app.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Cleanup DB Models, Repository, and Dedup Engine</name>
  <files>backend/db/models/cleanup.py, backend/db/models/__init__.py, backend/db/repositories/cleanup.py, backend/db/repositories/__init__.py, backend/dedup_engine.py</files>
  <action>
Create backend/db/models/cleanup.py with three ORM models (same pattern as existing models):

1. SubtitleHash:
   - id (Integer PK autoincrement)
   - file_path (Text, unique, not null)
   - content_hash (String(64), not null)
   - file_size (Integer, not null)
   - format (String(10), not null) -- "ass", "srt", "ssa"
   - language (String(10), nullable)
   - line_count (Integer, nullable)
   - last_scanned (Text, not null)
   - Indexes on content_hash and file_path

2. CleanupRule:
   - id (Integer PK autoincrement)
   - name (String(100), not null)
   - rule_type (String(50), not null) -- "dedup", "orphaned", "old_backups"
   - config_json (Text, default="{}")
   - enabled (Integer, default=1)
   - last_run_at (Text, nullable)
   - created_at (Text, not null)
   - updated_at (Text, not null)

3. CleanupHistory:
   - id (Integer PK autoincrement)
   - rule_id (Integer, nullable)
   - action_type (String(50), not null)
   - files_processed (Integer, default=0)
   - files_deleted (Integer, default=0)
   - bytes_freed (Integer, default=0)
   - details_json (Text, default="{}")
   - performed_at (Text, not null)
   - Index on performed_at

Register in db/models/__init__.py.

Create backend/db/repositories/cleanup.py with CleanupRepository(BaseRepository):
- Hashes: upsert_hash, get_hash_by_path, get_duplicate_groups (GROUP BY content_hash HAVING COUNT > 1), delete_hashes_by_paths, get_hash_stats
- Rules CRUD: create_rule, get_rules, update_rule, delete_rule, update_rule_last_run
- History: log_cleanup, get_history(page, per_page), get_cleanup_stats (total freed, total files)
- Disk analysis: get_disk_stats (total files, total size, by_format, duplicate_count, duplicate_size)

Register in db/repositories/__init__.py.

Create backend/dedup_engine.py:
1. compute_subtitle_hash(file_path: str) -> dict:
   - Read file content, normalize (strip, replace \\r\\n with \\n)
   - Compute SHA-256 of normalized content
   - Return {file_path, content_hash, file_size, format, language (extracted from filename pattern .lang.ext), line_count}

2. scan_for_duplicates(media_path: str, socketio=None) -> dict:
   - Walk media_path recursively, find all .srt, .ass, .ssa files
   - For each file: call compute_subtitle_hash, upsert to subtitle_hashes table
   - Emit WebSocket progress events (scan_progress with current/total counts) if socketio provided
   - Run file processing in ThreadPoolExecutor(max_workers=4) for parallelism
   - After scan: query duplicate groups from repository
   - Return {total_scanned, duplicates_found, groups: [{hash, files: [{path, size, format, language}]}]}

3. delete_duplicates(file_paths: list[str], keep_path: str) -> dict:
   - CRITICAL SAFETY: Verify keep_path is NOT in file_paths. Refuse if keep_path missing.
   - Delete each file in file_paths (os.remove)
   - Remove corresponding hashes from DB
   - Log to cleanup_history
   - Return {deleted: N, bytes_freed: N, errors: [...]}

4. scan_orphaned_subtitles(media_path: str) -> list:
   - Find subtitle files where parent media file no longer exists
   - A subtitle is orphaned if no .mkv/.mp4/.avi file shares its base name in the same directory
   - Return list of orphaned file paths with sizes

5. get_disk_space_analysis(media_path: str) -> dict:
   - From subtitle_hashes table: total_files, total_size_bytes, by_format (ass/srt/ssa counts and sizes)
   - duplicate_files, duplicate_size_bytes (from duplicate groups)
   - potential_savings_bytes
   - Include trend data from cleanup_history (last 30 days of bytes_freed)
  </action>
  <verify>
Run: cd backend && python -c "from db.models.cleanup import SubtitleHash, CleanupRule, CleanupHistory; print('models OK')"
Run: cd backend && python -c "from dedup_engine import compute_subtitle_hash, scan_for_duplicates; print('engine OK')"
  </verify>
  <done>Cleanup models, repository, and dedup engine importable with hash computation, duplicate detection, and safe deletion</done>
</task>

<task type="auto">
  <name>Task 2: Cleanup API Blueprint and Scheduler</name>
  <files>backend/routes/cleanup.py, backend/routes/__init__.py, backend/cleanup_scheduler.py</files>
  <action>
Create backend/routes/cleanup.py with Flask Blueprint at url_prefix="/api/v1/cleanup".

Endpoints:
1. Deduplication:
   - POST /scan -- Start background dedup scan. Run in daemon thread (same pattern as wanted search). Emit progress via WebSocket. Return {status: "scanning", scan_id: uuid}.
   - GET /scan/status -- Get current scan status (running/idle, progress %)
   - GET /duplicates -- Get duplicate groups from last scan (paginated)
   - POST /duplicates/delete -- Delete selected files from duplicate groups. Body: {groups: [{keep: path, delete: [paths]}]}. Enforce keep-at-least-one per group (400 if violated).

2. Orphaned Subtitles:
   - POST /orphaned/scan -- Scan for orphaned subtitles
   - GET /orphaned -- Get list of orphaned subtitle files
   - POST /orphaned/delete -- Delete selected orphaned files

3. Cleanup Rules:
   - GET /rules -- List all cleanup rules
   - POST /rules -- Create rule
   - PUT /rules/<id> -- Update rule
   - DELETE /rules/<id> -- Delete rule
   - POST /rules/<id>/run -- Execute a rule manually

4. Cleanup Dashboard:
   - GET /stats -- Disk space analysis (total files, sizes, duplicates, trends)
   - GET /history -- Cleanup execution history (paginated)

5. Preview:
   - POST /preview -- Preview what a cleanup operation would do without executing. Body: {action: "dedup"|"orphaned"|"rule", params: {...}}. Return list of affected files with sizes.

Register blueprint in routes/__init__.py.

Create backend/cleanup_scheduler.py:
- start_cleanup_scheduler(app, socketio) -- follows exact pattern of wanted_scanner scheduler in app.py
- Reads cleanup_schedule_interval_hours from config_entries (default: 168 = weekly)
- Runs enabled cleanup rules in order: dedup scan then rule execution
- Logs results to cleanup_history
- Uses APScheduler or simple threading.Timer pattern (match existing app.py scheduler approach)

Wire cleanup_scheduler into app.py startup (add import and call in create_app, same location as wanted_scanner scheduler start).
  </action>
  <verify>
Run: cd backend && python -c "from routes.cleanup import bp; print(bp.name, bp.url_prefix)"
Expect: "cleanup /api/v1/cleanup"
Run: cd backend && python -c "from cleanup_scheduler import start_cleanup_scheduler; print('scheduler OK')"
  </verify>
  <done>Cleanup Blueprint registered with all endpoints, scheduler wired into app startup</done>
</task>

</tasks>

<verification>
- Import test: from db.models.cleanup import SubtitleHash, CleanupRule, CleanupHistory
- Import test: from db.repositories.cleanup import CleanupRepository
- Import test: from dedup_engine import compute_subtitle_hash, scan_for_duplicates, delete_duplicates
- Import test: from routes.cleanup import bp
- Import test: from cleanup_scheduler import start_cleanup_scheduler
- Verify routes/__init__.py includes cleanup_bp
- Verify delete_duplicates refuses to delete the keep_path (safety guard)
</verification>

<success_criteria>
- SHA-256 hashing correctly identifies duplicate subtitle files
- Duplicate groups returned with file metadata for UI display
- Keep-at-least-one guard prevents accidental total deletion
- Background scan emits WebSocket progress events
- Cleanup rules stored and executable on schedule
- Disk space analysis returns actionable statistics
</success_criteria>

<output>
After completion, create .planning/phases/15-api-key-mgmt-notifications-cleanup/15-03-SUMMARY.md
</output>
