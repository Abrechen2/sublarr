---
phase: 10-performance-scalability
plan: 04
type: execute
wave: 2
depends_on: ["10-01"]
files_modified:
  - backend/cache/__init__.py
  - backend/cache/redis_cache.py
  - backend/cache/sqlite_cache.py
  - backend/queue/__init__.py
  - backend/queue/rq_queue.py
  - backend/queue/memory_queue.py
autonomous: true

must_haves:
  truths:
    - "User with Redis configured gets faster provider cache lookups via Redis instead of SQLite"
    - "User without Redis gets identical behavior to before (in-memory cache, in-process job queue)"
    - "Translation and wanted search jobs survive container restarts when Redis+RQ is configured"
    - "Factory functions auto-detect Redis availability and fall back gracefully with log messages"
  artifacts:
    - path: "backend/cache/__init__.py"
      provides: "CacheBackend ABC and create_cache_backend factory"
      contains: "class CacheBackend"
    - path: "backend/cache/redis_cache.py"
      provides: "RedisCacheBackend implementation"
      contains: "class RedisCacheBackend"
    - path: "backend/cache/sqlite_cache.py"
      provides: "MemoryCacheBackend fallback implementation (in-process dict with TTL)"
      contains: "class MemoryCacheBackend"
    - path: "backend/queue/__init__.py"
      provides: "QueueBackend ABC and create_job_queue factory"
      contains: "class QueueBackend"
    - path: "backend/queue/rq_queue.py"
      provides: "RQJobQueue with Redis persistence"
      contains: "class RQJobQueue"
    - path: "backend/queue/memory_queue.py"
      provides: "MemoryJobQueue with ThreadPoolExecutor fallback"
      contains: "class MemoryJobQueue"
  key_links:
    - from: "backend/cache/__init__.py"
      to: "backend/cache/redis_cache.py"
      via: "factory function creates RedisCacheBackend when Redis available"
      pattern: "RedisCacheBackend"
    - from: "backend/cache/__init__.py"
      to: "backend/cache/sqlite_cache.py"
      via: "factory function falls back to MemoryCacheBackend"
      pattern: "MemoryCacheBackend"
    - from: "backend/queue/__init__.py"
      to: "backend/queue/rq_queue.py"
      via: "factory function creates RQJobQueue when Redis available"
      pattern: "RQJobQueue"
    - from: "backend/queue/__init__.py"
      to: "backend/queue/memory_queue.py"
      via: "factory function falls back to MemoryJobQueue"
      pattern: "MemoryJobQueue"
---

<objective>
Create cache and job queue abstraction layers with Redis backends and graceful in-memory fallbacks. These are standalone packages that will be wired into the application in Plan 05 (app factory) and Plan 08 (actual usage integration).

Purpose: Redis is optional -- users who configure SUBLARR_REDIS_URL get Redis-backed caching and persistent job queues that survive container restarts. Users without Redis get the same functionality via in-memory cache and in-process ThreadPoolExecutor queue (matching current behavior). This satisfies PERF-04 (Redis cache), PERF-06 (RQ job queue), and PERF-09 (graceful degradation).

NOTE on PERF-05 (Sessions + Rate Limiting): Deferred to a future phase. The current app uses stateless API-key auth (no sessions) and has no rate limiting beyond provider-level circuit breakers. Adding Flask-Session and Flask-Limiter would require significant additional wiring with minimal benefit at current scale. The cache abstraction created here can serve as the Redis backend for both when they are needed.

Output: backend/cache/ and backend/queue/ packages with ABC interfaces, two implementations each, and factory functions.
</objective>

<execution_context>
@C:/Users/Dennis Wittke/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Dennis Wittke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-performance-scalability/10-RESEARCH.md
@.planning/phases/10-performance-scalability/10-01-SUMMARY.md
@backend/db/cache.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Cache abstraction layer (Redis + in-memory fallback)</name>
  <files>
    backend/cache/__init__.py
    backend/cache/redis_cache.py
    backend/cache/sqlite_cache.py
  </files>
  <action>
1. Create backend/cache/__init__.py:
   - Define CacheBackend ABC with methods:
     - get(key: str) -> Optional[str]: Get cached value by key.
     - set(key: str, value: str, ttl_seconds: int = 0) -> None: Set value with optional TTL.
     - delete(key: str) -> bool: Delete a key.
     - exists(key: str) -> bool: Check if key exists.
     - clear(prefix: str = "") -> int: Clear all keys (or keys matching prefix). Return count deleted.
     - get_stats() -> dict: Return cache statistics (hits, misses, size, backend type).
   - Define create_cache_backend(redis_url: str = "") -> CacheBackend factory function:
     - If redis_url is non-empty:
       - Try to import redis library.
       - Try to connect with socket_connect_timeout=5.
       - Try client.ping() to verify connectivity.
       - If all succeed: return RedisCacheBackend(client).
       - Log success: "Redis cache connected: {redis_url}".
     - On ImportError: log "redis package not installed, using memory cache".
     - On connection error: log warning "Redis unavailable ({error}), using memory cache".
     - Default: return MemoryCacheBackend().
   - Track cache statistics (hits, misses) as instance attributes on the ABC base.

2. Create backend/cache/redis_cache.py:
   - class RedisCacheBackend(CacheBackend):
     - __init__(self, redis_client): Store client reference as self.redis. Add "sublarr:" prefix to all keys for namespace isolation.
     - get(key): redis.get(prefixed_key), decode bytes to str. Increment hit/miss counters.
     - set(key, value, ttl_seconds): If ttl > 0: redis.setex(prefixed_key, ttl, value). Else: redis.set(prefixed_key, value).
     - delete(key): redis.delete(prefixed_key). Return True if deleted.
     - exists(key): redis.exists(prefixed_key).
     - clear(prefix): Use redis.scan_iter(match=f"sublarr:{prefix}*") and redis.delete() in batches. Return count.
     - get_stats(): Return dict with backend="redis", hits/misses, info from redis.info("stats") (used_memory, connected_clients, etc.).
   - Also support batch operations for future use:
     - mget(keys) -> dict[str, Optional[str]]: Redis MGET for multiple keys.
     - mset(mapping, ttl_seconds) -> None: Redis pipeline SETEX for multiple keys.

3. Create backend/cache/sqlite_cache.py (file named sqlite_cache.py for historical reasons, contains MemoryCacheBackend):
   - class MemoryCacheBackend(CacheBackend):
     - In-process dict with TTL eviction as the fallback.
     - __init__(): self._store = {} (key -> (value, expires_at_timestamp)).
     - get(key): Check expiry, return value or None. Lazy evict expired entries. Increment hit/miss.
     - set(key, value, ttl_seconds): Calculate expires_at from time.time() + ttl. Store in dict.
     - delete(key): Remove from dict. Return True if existed.
     - exists(key): Check dict and expiry.
     - clear(prefix): Delete matching keys from dict. Return count.
     - get_stats(): Return dict with backend="memory", hits/misses, size=len(self._store).
   - Add periodic cleanup: _evict_expired() called on every N-th access (e.g., every 100 accesses) to remove all expired entries and prevent memory growth.
   - Thread-safe: Use threading.Lock for dict access.
  </action>
  <verify>
Run `cd Z:/CC/Sublarr/backend && python -c "from cache import CacheBackend, create_cache_backend; c = create_cache_backend(); c.set('test', 'value', 60); assert c.get('test') == 'value'; c.delete('test'); assert c.get('test') is None; print('Cache fallback OK')"`.
Run `cd Z:/CC/Sublarr/backend && python -c "from cache.redis_cache import RedisCacheBackend; print('Redis cache importable')"`.
  </verify>
  <done>Cache abstraction with CacheBackend ABC, RedisCacheBackend, and MemoryCacheBackend fallback. Factory function auto-detects Redis availability. Hit/miss tracking for monitoring.</done>
</task>

<task type="auto">
  <name>Task 2: Job queue abstraction layer (RQ + ThreadPoolExecutor fallback)</name>
  <files>
    backend/queue/__init__.py
    backend/queue/rq_queue.py
    backend/queue/memory_queue.py
  </files>
  <action>
1. Create backend/queue/__init__.py:
   - Define JobStatus enum: QUEUED, RUNNING, COMPLETED, FAILED, CANCELLED.
   - Define JobInfo dataclass: id (str), func_name (str), status (JobStatus), enqueued_at (str), started_at (Optional[str]), completed_at (Optional[str]), result (Optional[any]), error (Optional[str]).
   - Define QueueBackend ABC with methods:
     - enqueue(func, *args, job_id: str = None, **kwargs) -> str: Submit function for execution. Return job_id.
     - get_job(job_id) -> Optional[JobInfo]: Get job status.
     - cancel_job(job_id) -> bool: Cancel a pending job.
     - get_queue_length() -> int: Number of pending jobs.
     - get_active_jobs() -> list[JobInfo]: Currently executing jobs.
     - get_failed_jobs(limit) -> list[JobInfo]: Failed jobs.
     - clear_failed() -> int: Clear failed job records.
     - get_backend_info() -> dict: Backend type, connection status, worker count.
   - Define create_job_queue(redis_url: str = "", queue_name: str = "sublarr") -> QueueBackend factory:
     - If redis_url is non-empty:
       - Try import redis and rq.
       - Connect to Redis.
       - Return RQJobQueue(redis_client, queue_name).
       - Log success.
     - On ImportError: log and fallback.
     - On connection error: log warning and fallback.
     - Default: return MemoryJobQueue().

2. Create backend/queue/rq_queue.py:
   - class RQJobQueue(QueueBackend):
     - __init__(self, redis_client, queue_name="sublarr"):
       - `from rq import Queue` (lazy import, guarded).
       - self.queue = Queue(queue_name, connection=redis_client).
       - self.redis_client = redis_client.
     - enqueue(func, *args, job_id=None, **kwargs) -> str:
       - job = self.queue.enqueue(func, *args, job_id=job_id, **kwargs).
       - Return job.id.
     - get_job(job_id) -> Optional[JobInfo]:
       - from rq.job import Job as RQJob.
       - try: job = RQJob.fetch(job_id, connection=self.redis_client).
       - Map RQ job status to our JobStatus enum.
       - Return JobInfo with all fields populated.
     - cancel_job(job_id) -> bool:
       - Fetch job, call job.cancel().
     - get_queue_length() -> int: len(self.queue).
     - get_active_jobs(): Query started registry.
     - get_failed_jobs(limit): Query failed registry.
     - clear_failed(): self.queue.failed_job_registry.cleanup().
     - get_backend_info(): Return {"type": "rq", "redis_url": "...", "queue_name": "...", "workers": ...}.
   - CRITICAL: RQ requires a separate worker process to execute jobs. The RQJobQueue only ENQUEUES jobs; a separate `rq worker` process must be running to EXECUTE them. For the Docker container, this means running both Flask and an RQ worker. Document this in the get_backend_info() output. The Plan 05 integration will handle the worker startup.

3. Create backend/queue/memory_queue.py:
   - class MemoryJobQueue(QueueBackend):
     - __init__(self, max_workers=2):
       - self._executor = ThreadPoolExecutor(max_workers=max_workers).
       - self._jobs: dict[str, dict] = {} (tracks job metadata).
       - self._lock = threading.Lock() (thread-safe job tracking).
     - enqueue(func, *args, job_id=None, **kwargs) -> str:
       - Generate job_id if not provided (uuid[:8]).
       - Record job metadata: {status: QUEUED, func_name, enqueued_at, future: None}.
       - Submit to executor: future = self._executor.submit(self._run_job, job_id, func, *args, **kwargs).
       - Store future in job metadata.
       - Add future.add_done_callback(self._on_complete) to update status on completion.
       - Return job_id.
     - _run_job(self, job_id, func, *args, **kwargs):
       - Update status to RUNNING, set started_at.
       - result = func(*args, **kwargs).
       - Return result.
     - _on_complete(self, job_id, future):
       - If future.exception(): status=FAILED, error=str(exception).
       - Else: status=COMPLETED, result=future.result().
       - Set completed_at.
     - get_job(job_id) -> Optional[JobInfo]: Return from self._jobs.
     - cancel_job(job_id) -> bool: future.cancel() if still pending.
     - get_queue_length() -> int: Count jobs with status QUEUED.
     - get_active_jobs(): Filter by RUNNING status.
     - get_failed_jobs(limit): Filter by FAILED status.
     - clear_failed(): Remove FAILED entries from self._jobs.
     - get_backend_info(): Return {"type": "memory", "max_workers": ..., "active": ..., "queued": ...}.
   - IMPORTANT: MemoryJobQueue does NOT persist across restarts (by design -- this is the tradeoff vs RQ). Jobs in flight when the container stops are lost. This matches current behavior (daemon threads).
   - Add periodic cleanup: old completed/failed jobs removed after 24h to prevent memory leak.
  </action>
  <verify>
Run `cd Z:/CC/Sublarr/backend && python -c "
from queue import create_job_queue, JobStatus
q = create_job_queue()  # No Redis = memory fallback
import time
def test_fn(x): return x * 2
job_id = q.enqueue(test_fn, 5)
time.sleep(0.5)
info = q.get_job(job_id)
print(f'Job {job_id}: status={info.status}, result={info.result}')
assert info.status == JobStatus.COMPLETED
assert info.result == 10
print('Memory queue OK')
"`.
Run `cd Z:/CC/Sublarr/backend && python -c "from queue.rq_queue import RQJobQueue; print('RQ queue importable')"`.
  </verify>
  <done>Queue abstraction with QueueBackend ABC, RQJobQueue (Redis persistent), and MemoryJobQueue (in-process fallback). Factory function auto-detects Redis/RQ availability. JobInfo dataclass provides unified status tracking.</done>
</task>

</tasks>

<verification>
1. Cache: create_cache_backend() returns working backend without Redis
2. Cache: RedisCacheBackend importable (even without Redis running)
3. Queue: create_job_queue() returns working MemoryJobQueue without Redis
4. Queue: RQJobQueue importable (even without Redis running)
5. Both factory functions log appropriate messages about backend selection
6. No existing code changed (new packages only)
</verification>

<success_criteria>
- Cache and queue ABCs with clean interfaces
- Redis implementations with proper error handling and connection management
- Fallback implementations that match current behavior (in-memory cache, ThreadPoolExecutor queue)
- Factory functions with graceful degradation and logging
- No Redis dependency in the critical path (always falls back)
</success_criteria>

<output>
After completion, create `.planning/phases/10-performance-scalability/10-04-SUMMARY.md`
</output>
