---
phase: 10-performance-scalability
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/requirements.txt
  - backend/extensions.py
  - backend/db/models/__init__.py
  - backend/db/models/core.py
  - backend/db/models/providers.py
  - backend/db/models/translation.py
  - backend/db/models/hooks.py
  - backend/db/models/standalone.py
  - backend/db/migrations/env.py
  - backend/db/migrations/alembic.ini
  - backend/db/migrations/script.py.mako
autonomous: true

must_haves:
  truths:
    - "SQLAlchemy ORM models exist for all 25+ database tables matching the existing SCHEMA DDL"
    - "Flask-SQLAlchemy and Flask-Migrate are initialized in extensions.py"
    - "Alembic migration infrastructure is configured with batch mode for SQLite compatibility"
    - "An initial migration exists that stamps existing databases at head without running DDL"
  artifacts:
    - path: "backend/db/models/__init__.py"
      provides: "Base DeclarativeBase class, common imports, all model re-exports"
      contains: "class Base(DeclarativeBase)"
    - path: "backend/db/models/core.py"
      provides: "Job, DailyStats, ConfigEntry, UpgradeHistory, LanguageProfile, SeriesLanguageProfile, MovieLanguageProfile, FfprobeCache, BlacklistEntry models"
      min_lines: 150
    - path: "backend/db/models/providers.py"
      provides: "ProviderCache, SubtitleDownload, ProviderStats, ProviderScoreModifier, ScoringWeights models"
      min_lines: 80
    - path: "backend/db/models/translation.py"
      provides: "TranslationConfigHistory, GlossaryEntry, PromptPreset, TranslationBackendStats, WhisperJob models"
      min_lines: 80
    - path: "backend/db/models/hooks.py"
      provides: "HookConfig, WebhookConfig, HookLog models"
      min_lines: 60
    - path: "backend/db/models/standalone.py"
      provides: "WatchedFolder, StandaloneSeries, StandaloneMovie, MetadataCache, AnidbMapping models"
      min_lines: 60
    - path: "backend/db/migrations/env.py"
      provides: "Alembic env with render_as_batch=True and stamp-existing-db logic"
      contains: "render_as_batch"
    - path: "backend/extensions.py"
      provides: "SQLAlchemy db instance and Migrate instance alongside existing socketio"
      contains: "SQLAlchemy"
  key_links:
    - from: "backend/extensions.py"
      to: "flask_sqlalchemy"
      via: "SQLAlchemy() instantiation"
      pattern: "db = SQLAlchemy"
    - from: "backend/db/models/__init__.py"
      to: "backend/extensions.py"
      via: "imports db for Base metadata binding"
      pattern: "from extensions import db"
    - from: "backend/db/migrations/env.py"
      to: "backend/db/models/__init__.py"
      via: "target_metadata for autogenerate"
      pattern: "target_metadata"
---

<objective>
Create SQLAlchemy ORM model definitions for all 25+ database tables and set up Alembic migration infrastructure with Flask-Migrate.

Purpose: This is the foundation for the entire Performance & Scalability phase. ORM models define the schema in a dialect-agnostic way (SQLite + PostgreSQL), and Alembic provides versioned migrations that replace the manual _run_migrations() function. No existing code is changed in this plan -- only new files are created alongside the existing raw sqlite3 layer.

Output: db/models/ package with all ORM models, db/migrations/ directory with Alembic config, updated extensions.py with Flask-SQLAlchemy + Flask-Migrate instances, updated requirements.txt with new dependencies.
</objective>

<execution_context>
@C:/Users/Dennis Wittke/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Dennis Wittke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-performance-scalability/10-RESEARCH.md
@backend/db/__init__.py
@backend/extensions.py
@backend/config.py
@backend/requirements.txt
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add dependencies and create SQLAlchemy ORM models for all tables</name>
  <files>
    backend/requirements.txt
    backend/db/models/__init__.py
    backend/db/models/core.py
    backend/db/models/providers.py
    backend/db/models/translation.py
    backend/db/models/hooks.py
    backend/db/models/standalone.py
  </files>
  <action>
1. Add to requirements.txt (in appropriate sections):
   - SQLAlchemy==2.0.46
   - Flask-SQLAlchemy==3.1.1
   - Flask-Migrate==4.1.0
   - alembic==1.18.4
   - psycopg2-binary==2.9.10 (comment: "# Optional: PostgreSQL support")
   - redis==7.1.0 (comment: "# Optional: Redis caching/queue")
   - rq==2.6.1 (comment: "# Optional: Redis-based job queue")
   - fakeredis==2.26.2 (add near testing deps, comment: "# Testing: Redis mock")

2. Create backend/db/models/__init__.py:
   - Import Base from extensions (from extensions import db; Base = db.Model -- this is the Flask-SQLAlchemy way)
   - Actually, use standalone DeclarativeBase since Flask-SQLAlchemy 3.1 supports custom bases. Define: `class Base(db.Model): __abstract__ = True` OR simply use `db.Model` as the base. The simplest approach: all models inherit from `db.Model` where `db` is the Flask-SQLAlchemy instance.
   - Re-export all models from submodules for convenient importing: `from db.models.core import *` etc.
   - This file should import all model classes so Alembic autogenerate can detect them.

3. Create backend/db/models/core.py with models matching SCHEMA DDL exactly:
   - Job: id (String(8) PK), file_path (Text), status (String(20)), source_format (String(10)), output_path (Text), stats_json (Text), error (Text), force (Integer), bazarr_context_json (Text), config_hash (String(12)), created_at (Text), completed_at (Text). Indexes: idx_jobs_status, idx_jobs_created.
   - DailyStats: date (Text PK), translated (Integer), failed (Integer), skipped (Integer), by_format_json (Text), by_source_json (Text).
   - ConfigEntry: key (Text PK), value (Text), updated_at (Text).
   - WantedItem: id (Integer PK auto), item_type (String(20)), sonarr_series_id (Integer nullable), sonarr_episode_id (Integer nullable), radarr_movie_id (Integer nullable), title (Text), season_episode (Text), file_path (Text), existing_sub (Text), missing_languages (Text), status (String(20)), last_search_at (Text), search_count (Integer), error (Text), added_at (Text), updated_at (Text), upgrade_candidate (Integer), current_score (Integer), target_language (Text), instance_name (Text), standalone_series_id (Integer nullable), standalone_movie_id (Integer nullable), subtitle_type (String(20)). Indexes: status, item_type, file_path, sonarr_series_id, sonarr_episode_id, radarr_movie_id.
   - UpgradeHistory: id (Integer PK auto), file_path (Text), old_format (Text), old_score (Integer), new_format (Text), new_score (Integer), provider_name (Text), upgrade_reason (Text), upgraded_at (Text). Index: file_path.
   - LanguageProfile: id (Integer PK auto), name (Text UNIQUE), source_language (Text), source_language_name (Text), target_languages_json (Text), target_language_names_json (Text), is_default (Integer), translation_backend (Text), fallback_chain_json (Text), forced_preference (Text), created_at (Text), updated_at (Text).
   - SeriesLanguageProfile: sonarr_series_id (Integer PK), profile_id (Integer FK->language_profiles.id).
   - MovieLanguageProfile: radarr_movie_id (Integer PK), profile_id (Integer FK->language_profiles.id).
   - FfprobeCache: file_path (Text PK), mtime (Float), probe_data_json (Text), cached_at (Text). Index: mtime.
   - BlacklistEntry: id (Integer PK auto), provider_name (Text), subtitle_id (Text), language (Text), file_path (Text), title (Text), reason (Text), added_at (Text). UniqueConstraint: (provider_name, subtitle_id). Index: (provider_name, subtitle_id).

   IMPORTANT: Use Text type (not DateTime) for all timestamp columns to match existing TEXT-based schema. This preserves backward compatibility. DateTime migration can happen later.

   IMPORTANT: Use SQLAlchemy's JSON type for columns that store JSON but are currently TEXT. This maps to TEXT on SQLite and JSONB on PostgreSQL automatically. Apply to: stats_json, by_format_json, by_source_json, bazarr_context_json, missing_languages, target_languages_json, target_language_names_json, fallback_chain_json, probe_data_json. But keep them as Text for now to avoid data migration issues -- the JSON type change can be a future migration.

4. Create backend/db/models/providers.py:
   - ProviderCache: id (Integer PK auto), provider_name (Text), query_hash (Text), results_json (Text), cached_at (Text), expires_at (Text). Index: (provider_name, query_hash), expires_at.
   - SubtitleDownload: id (Integer PK auto), provider_name (Text), subtitle_id (Text), language (Text), format (Text), file_path (Text), score (Integer), subtitle_type (Text), downloaded_at (Text). Index: file_path.
   - ProviderStats: provider_name (Text PK), total_searches (Integer), successful_downloads (Integer), failed_downloads (Integer), avg_score (Float), last_success_at (Text), last_failure_at (Text), consecutive_failures (Integer), avg_response_time_ms (Float), last_response_time_ms (Float), auto_disabled (Integer), disabled_until (Text), updated_at (Text). Index: updated_at.
   - ProviderScoreModifier: provider_name (Text PK), modifier (Integer), updated_at (Text).
   - ScoringWeights: id (Integer PK auto), score_type (Text), weight_key (Text), weight_value (Integer), updated_at (Text). UniqueConstraint: (score_type, weight_key).

5. Create backend/db/models/translation.py:
   - TranslationConfigHistory: id (Integer PK auto), config_hash (Text UNIQUE), ollama_model (Text), prompt_template (Text), target_language (Text), first_used_at (Text), last_used_at (Text).
   - GlossaryEntry: id (Integer PK auto), series_id (Integer), source_term (Text), target_term (Text), notes (Text), created_at (Text), updated_at (Text). Indexes: series_id, source_term.
   - PromptPreset: id (Integer PK auto), name (Text UNIQUE), prompt_template (Text), is_default (Integer), created_at (Text), updated_at (Text).
   - TranslationBackendStats: backend_name (Text PK), total_requests (Integer), successful_translations (Integer), failed_translations (Integer), total_characters (Integer), avg_response_time_ms (Float), last_response_time_ms (Float), last_success_at (Text), last_failure_at (Text), last_error (Text), consecutive_failures (Integer), updated_at (Text). Index: updated_at.
   - WhisperJob: id (Text PK), file_path (Text), language (Text), status (String(20)), progress (Float), phase (Text), backend_name (Text), detected_language (Text), language_probability (Float), srt_content (Text), segment_count (Integer), duration_seconds (Float), processing_time_ms (Float), error (Text), created_at (Text), started_at (Text), completed_at (Text). Indexes: status, created_at.

6. Create backend/db/models/hooks.py:
   - HookConfig: id (Integer PK auto), name (Text), event_name (Text), hook_type (String(20)), enabled (Integer), script_path (Text), timeout_seconds (Integer), last_triggered_at (Text), last_status (Text), trigger_count (Integer), created_at (Text), updated_at (Text). Index: event_name.
   - WebhookConfig: id (Integer PK auto), name (Text), event_name (Text), url (Text), secret (Text), enabled (Integer), retry_count (Integer), timeout_seconds (Integer), last_triggered_at (Text), last_status_code (Integer), last_error (Text), consecutive_failures (Integer), trigger_count (Integer), created_at (Text), updated_at (Text). Index: event_name.
   - HookLog: id (Integer PK auto), hook_id (Integer nullable), webhook_id (Integer nullable), event_name (Text), hook_type (Text), success (Integer), exit_code (Integer nullable), status_code (Integer nullable), stdout (Text), stderr (Text), error (Text), duration_ms (Float), triggered_at (Text). Indexes: hook_id, webhook_id, triggered_at.

7. Create backend/db/models/standalone.py:
   - WatchedFolder: id (Integer PK auto), path (Text UNIQUE), label (Text), media_type (Text), enabled (Integer), last_scan_at (Text), created_at (Text), updated_at (Text).
   - StandaloneSeries: id (Integer PK auto), title (Text), year (Integer nullable), folder_path (Text UNIQUE), tmdb_id (Integer nullable), tvdb_id (Integer nullable), anilist_id (Integer nullable), imdb_id (Text), poster_url (Text), is_anime (Integer), episode_count (Integer), season_count (Integer), metadata_source (Text), created_at (Text), updated_at (Text). Indexes: tmdb_id, anilist_id.
   - StandaloneMovie: id (Integer PK auto), title (Text), year (Integer nullable), file_path (Text UNIQUE), tmdb_id (Integer nullable), imdb_id (Text), poster_url (Text), metadata_source (Text), created_at (Text), updated_at (Text). Index: tmdb_id.
   - MetadataCache: cache_key (Text PK), provider (Text), response_json (Text), cached_at (Text), expires_at (Text). Index: expires_at.
   - AnidbMapping: tvdb_id (Integer PK), anidb_id (Integer), series_title (Text nullable), created_at (Text), last_used (Text). Index: anidb_id.

All models use `__tablename__` matching the existing table names exactly. All column types and defaults match the existing SCHEMA DDL. Use `mapped_column()` with `Mapped[]` annotations (SQLAlchemy 2.0 style).

Each model module has `__all__` listing its model classes. The __init__.py imports all and re-exports them.
  </action>
  <verify>
Run `cd Z:/CC/Sublarr/backend && python -c "from db.models import *; print('All models imported successfully')"` -- should print success message.
Run `cd Z:/CC/Sublarr/backend && python -c "from db.models.core import Job, WantedItem, ConfigEntry; print(Job.__tablename__, WantedItem.__tablename__)"` -- should print "jobs wanted_items".
  </verify>
  <done>All 25+ ORM models defined in db/models/ package, matching existing schema exactly. All models importable without errors. requirements.txt updated with all new dependencies.</done>
</task>

<task type="auto">
  <name>Task 2: Flask-SQLAlchemy extension + Alembic migration infrastructure</name>
  <files>
    backend/extensions.py
    backend/db/migrations/env.py
    backend/db/migrations/alembic.ini
    backend/db/migrations/script.py.mako
    backend/db/migrations/versions/.gitkeep
  </files>
  <action>
1. Update backend/extensions.py:
   - Keep existing socketio import and instance.
   - Add Flask-SQLAlchemy: `from flask_sqlalchemy import SQLAlchemy` with `db = SQLAlchemy()`.
   - Add Flask-Migrate: `from flask_migrate import Migrate` with `migrate = Migrate()`.
   - Guard imports with try/except ImportError for graceful degradation during transition (existing code that imports socketio should not break if SQLAlchemy is not yet installed).

2. Create backend/db/migrations/alembic.ini:
   - Standard Alembic config pointing to the migrations directory.
   - script_location = backend/db/migrations (relative to project root).
   - sqlalchemy.url will be overridden programmatically by env.py.

3. Create backend/db/migrations/env.py:
   - Import Flask app context and db from extensions.
   - Set target_metadata to db.metadata (which has all model metadata).
   - Import all models at top: `from db.models import *` (ensures Alembic sees all tables).
   - Configure render_as_batch=True for all migration contexts (CRITICAL for SQLite ALTER TABLE compatibility).
   - Implement run_migrations_online() that gets engine from current_app.extensions.
   - Add stamp_existing_db_if_needed() function:
     - Check if alembic_version table exists
     - If NOT exists but other tables DO exist (existing DB without Alembic) -> stamp as 'head'
     - If neither exists (fresh DB) -> do nothing (upgrade will create everything)
     - This prevents "Table already exists" errors on existing installations.

4. Create backend/db/migrations/script.py.mako:
   - Standard Alembic migration template with batch mode import.

5. Create backend/db/migrations/versions/.gitkeep:
   - Empty directory for migration version files.
   - Note: Do NOT create an initial migration at this point. The initial migration will be created in Plan 05 when the full integration happens. For now, we just set up the infrastructure.

IMPORTANT: Do NOT modify app.py in this plan. The Flask-SQLAlchemy/Migrate initialization into the app factory happens in Plan 05 (integration wiring). This plan only creates the instances and infrastructure.
  </action>
  <verify>
Run `cd Z:/CC/Sublarr/backend && python -c "from extensions import socketio, db, migrate; print('Extensions loaded:', type(db).__name__, type(migrate).__name__)"` -- should print "Extensions loaded: SQLAlchemy Migrate".
Verify file exists: `ls Z:/CC/Sublarr/backend/db/migrations/env.py Z:/CC/Sublarr/backend/db/migrations/alembic.ini`.
Run `cd Z:/CC/Sublarr/backend && python -c "from db.models import Base; print('Tables:', len(Base.metadata.tables))"` -- should print a count >= 25.
  </verify>
  <done>Flask-SQLAlchemy db and Flask-Migrate migrate instances exist in extensions.py. Alembic migration infrastructure configured with batch mode and stamp-existing-db logic. All ORM model metadata discoverable by Alembic autogenerate.</done>
</task>

</tasks>

<verification>
1. All 25+ ORM models importable: `python -c "from db.models import *"`
2. Extensions export db and migrate: `python -c "from extensions import db, migrate"`
3. Model table names match existing schema: compare `[m.__tablename__ for m in ...]` against SCHEMA DDL
4. No existing tests break (models are additive, no code changed): `cd backend && python -m pytest tests/ -x --timeout=30`
5. requirements.txt includes all new dependencies
</verification>

<success_criteria>
- ORM models for all 25+ tables exist in backend/db/models/ with correct column types and indexes
- Flask-SQLAlchemy and Flask-Migrate instances in extensions.py
- Alembic migration infrastructure with batch mode and stamp logic
- No existing functionality broken (additive changes only)
</success_criteria>

<output>
After completion, create `.planning/phases/10-performance-scalability/10-01-SUMMARY.md`
</output>
