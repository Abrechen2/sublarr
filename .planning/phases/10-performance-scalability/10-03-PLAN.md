---
phase: 10-performance-scalability
plan: 03
type: execute
wave: 2
depends_on: ["10-01"]
files_modified:
  - backend/db/repositories/jobs.py
  - backend/db/repositories/wanted.py
  - backend/db/repositories/profiles.py
  - backend/db/repositories/providers.py
  - backend/db/repositories/hooks.py
  - backend/db/repositories/standalone.py
autonomous: true

must_haves:
  truths:
    - "Complex repository classes provide identical functionality to the 6 larger db/ modules"
    - "Wanted repository handles multi-column upsert logic (file_path + target_language + subtitle_type)"
    - "Provider repository preserves weighted running average for response time statistics"
    - "Hooks repository handles the hook_configs, webhook_configs, and hook_log tables together"
  artifacts:
    - path: "backend/db/repositories/jobs.py"
      provides: "JobRepository replacing db/jobs.py (275 lines of CRUD + daily stats)"
      min_lines: 180
    - path: "backend/db/repositories/wanted.py"
      provides: "WantedRepository replacing db/wanted.py (356 lines of complex upsert + status tracking)"
      min_lines: 250
    - path: "backend/db/repositories/profiles.py"
      provides: "ProfileRepository replacing db/profiles.py (303 lines of profile CRUD + series/movie mapping)"
      min_lines: 200
    - path: "backend/db/repositories/providers.py"
      provides: "ProviderRepository replacing db/providers.py (368 lines of stats + auto-disable logic)"
      min_lines: 250
    - path: "backend/db/repositories/hooks.py"
      provides: "HookRepository replacing db/hooks.py (446 lines of hook/webhook/log CRUD)"
      min_lines: 300
    - path: "backend/db/repositories/standalone.py"
      provides: "StandaloneRepository replacing db/standalone.py (408 lines of watched folders + series/movie CRUD)"
      min_lines: 280
  key_links:
    - from: "backend/db/repositories/jobs.py"
      to: "backend/db/models/core.py"
      via: "Job and DailyStats model imports"
      pattern: "from db.models.core import Job, DailyStats"
    - from: "backend/db/repositories/wanted.py"
      to: "backend/db/models/core.py"
      via: "WantedItem model import"
      pattern: "from db.models.core import WantedItem"
    - from: "backend/db/repositories/providers.py"
      to: "backend/db/models/providers.py"
      via: "ProviderStats model import"
      pattern: "from db.models.providers import ProviderStats"
---

<objective>
Convert the 6 larger and more complex db/ domain modules to the Repository pattern using SQLAlchemy ORM sessions. These modules have the most intricate query logic: multi-column upserts, aggregate statistics, weighted averages, cascading operations, and cross-table queries.

Purpose: Completing the repository layer means ALL database operations have SQLAlchemy equivalents, enabling the PostgreSQL switch in Plan 05. These modules are split from Plan 02 because they contain complex query patterns that need more careful conversion.

Output: 6 repository classes in backend/db/repositories/ covering jobs, wanted, profiles, providers, hooks, and standalone.
</objective>

<execution_context>
@C:/Users/Dennis Wittke/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Dennis Wittke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-performance-scalability/10-RESEARCH.md
@.planning/phases/10-performance-scalability/10-01-SUMMARY.md
@backend/db/jobs.py
@backend/db/wanted.py
@backend/db/profiles.py
@backend/db/providers.py
@backend/db/hooks.py
@backend/db/standalone.py
@backend/db/repositories/base.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Jobs, wanted, and profiles repositories</name>
  <files>
    backend/db/repositories/jobs.py
    backend/db/repositories/wanted.py
    backend/db/repositories/profiles.py
  </files>
  <action>
1. Create backend/db/repositories/jobs.py (replacing 275-line db/jobs.py):
   - class JobRepository(BaseRepository):
     - create_job(file_path, force, arr_context) -> dict: Generate uuid[:8], INSERT Job with 'queued' status.
     - update_job(job_id, status, result, error): Extract stats_json, output_path, source_format, config_hash from result dict. Update completed_at only for terminal statuses.
     - get_job(job_id) -> Optional[dict]
     - get_jobs(status, limit, offset) -> list[dict]
     - get_recent_jobs(limit) -> list[dict]: Ordered by created_at DESC.
     - get_pending_job_count() -> int: Count where status IN ('queued', 'running').
     - delete_job(job_id) -> bool
     - delete_old_jobs(days) -> int: Delete jobs older than N days.
     - record_daily_stats(status, fmt, source): Upsert into daily_stats for today's date. Increment translated/failed/skipped counters. Update by_format_json and by_source_json (JSON merge).
     - get_daily_stats(days) -> list[dict]: Last N days.
     - get_stats_summary() -> dict: Aggregate totals from daily_stats.
   - CRITICAL: record_daily_stats merges JSON dicts. Read the existing implementation carefully -- it does `json.loads(existing_json)`, updates the dict, then `json.dumps()` back. With SQLAlchemy, load the model, modify the JSON columns, save. For PostgreSQL JSONB, this is natively supported.
   - Helper: _row_to_job(model) -> dict: Convert Job model to dict matching existing format.

2. Create backend/db/repositories/wanted.py (replacing 356-line db/wanted.py):
   - class WantedRepository(BaseRepository):
     - upsert_wanted_item(item_type, file_path, title, season_episode, existing_sub, missing_languages, sonarr_series_id, sonarr_episode_id, radarr_movie_id, standalone_series_id, standalone_movie_id, upgrade_candidate, current_score, target_language, instance_name, subtitle_type) -> int: Complex upsert matching on file_path + target_language + subtitle_type. If existing item found in 'completed'/'failed'/'blacklisted' status, update it back to 'wanted'. If existing item in 'wanted' status, update metadata only.
     - get_wanted_items(status, item_type, limit, offset) -> list[dict]
     - get_wanted_item(item_id) -> Optional[dict]
     - get_wanted_by_file_path(file_path, target_language, subtitle_type) -> Optional[dict]
     - update_wanted_status(item_id, status, error) -> bool
     - mark_search_attempted(item_id) -> bool: Increment search_count, set last_search_at.
     - get_wanted_summary() -> dict: Counts by status.
     - get_wanted_for_series(sonarr_series_id) -> list[dict]
     - get_wanted_for_movie(radarr_movie_id) -> list[dict]
     - delete_wanted_item(item_id) -> bool
     - delete_wanted_by_file_path(file_path) -> int
     - cleanup_wanted_items(instance_name) -> int: Remove items for missing file paths.
     - get_wanted_by_subtitle_type(subtitle_type) -> list[dict]
   - CRITICAL: The upsert logic is the most complex function in the entire db layer. Read db/wanted.py upsert_wanted_item() line by line. It has conditional WHERE clauses based on whether target_language is provided. Preserve this exact logic.
   - Helper: _row_to_wanted(model) -> dict: Convert WantedItem model to dict. Parse missing_languages JSON string back to list.

3. Create backend/db/repositories/profiles.py (replacing 303-line db/profiles.py):
   - class ProfileRepository(BaseRepository):
     - create_profile(name, source_language, source_language_name, target_languages_json, target_language_names_json, is_default, translation_backend, fallback_chain_json, forced_preference) -> dict
     - get_profiles() -> list[dict]
     - get_profile(profile_id) -> Optional[dict]
     - get_default_profile() -> Optional[dict]
     - update_profile(profile_id, **kwargs) -> bool
     - delete_profile(profile_id) -> bool: Also cascade-delete series/movie mappings.
     - set_series_profile(sonarr_series_id, profile_id)
     - get_series_profile(sonarr_series_id) -> Optional[dict]: Join with language_profiles to return full profile.
     - remove_series_profile(sonarr_series_id) -> bool
     - set_movie_profile(radarr_movie_id, profile_id)
     - get_movie_profile(radarr_movie_id) -> Optional[dict]: Join with language_profiles.
     - remove_movie_profile(radarr_movie_id) -> bool
     - get_profile_assignments() -> dict: Return {series: [...], movies: [...]} for all assignments.
   - Profile CRUD with cascade to SeriesLanguageProfile and MovieLanguageProfile.
   - Helper: _row_to_profile(model) -> dict: Parse JSON columns (target_languages_json etc) back to lists.
  </action>
  <verify>
Run `cd Z:/CC/Sublarr/backend && python -c "from db.repositories.jobs import JobRepository; print('JobRepository OK')"`.
Run `cd Z:/CC/Sublarr/backend && python -c "from db.repositories.wanted import WantedRepository; print('WantedRepository OK')"`.
Run `cd Z:/CC/Sublarr/backend && python -c "from db.repositories.profiles import ProfileRepository; print('ProfileRepository OK')"`.
  </verify>
  <done>Jobs, wanted, and profiles repositories created with all functions matching existing db/ module signatures and return shapes. Complex upsert and cascade delete logic preserved.</done>
</task>

<task type="auto">
  <name>Task 2: Providers, hooks, and standalone repositories</name>
  <files>
    backend/db/repositories/providers.py
    backend/db/repositories/hooks.py
    backend/db/repositories/standalone.py
  </files>
  <action>
1. Create backend/db/repositories/providers.py (replacing 368-line db/providers.py):
   - class ProviderRepository(BaseRepository):
     - record_search(provider_name, success, response_time_ms): Upsert ProviderStats. Increment total_searches, update success/failure timestamps, update consecutive_failures (reset on success, increment on failure). Update avg_response_time_ms using weighted running average: `(old_avg * (n-1) + new) / n`.
     - record_download(provider_name, score): Increment successful_downloads, update avg_score using weighted running average.
     - record_download_failure(provider_name): Increment failed_downloads.
     - get_provider_stats(provider_name) -> Optional[dict]
     - get_all_provider_stats() -> list[dict]
     - clear_provider_stats(provider_name) -> bool
     - check_auto_disable(provider_name, threshold) -> bool: Check if consecutive_failures >= threshold. If so, set auto_disabled=1 and disabled_until.
     - clear_auto_disable(provider_name) -> bool: Reset auto_disabled=0, disabled_until='', consecutive_failures=0.
     - is_auto_disabled(provider_name) -> bool: Check auto_disabled=1 AND disabled_until > now (or disabled_until is empty meaning permanently disabled until cleared).
     - get_disabled_providers() -> list[dict]
   - CRITICAL: The weighted running average for avg_response_time_ms MUST match existing formula: `(old_avg * (total_searches - 1) + new_time) / total_searches`. Read db/providers.py record_search_result() and record_response_time() to get the exact formula.
   - Helper: _row_to_stats(model) -> dict: Convert ProviderStats model to dict.

2. Create backend/db/repositories/hooks.py (replacing 446-line db/hooks.py):
   - class HookRepository(BaseRepository):
     Hook configs:
     - create_hook(name, event_name, hook_type, script_path, timeout_seconds, enabled) -> dict
     - get_hooks(event_name, enabled_only) -> list[dict]
     - get_hook(hook_id) -> Optional[dict]
     - update_hook(hook_id, **kwargs) -> bool
     - delete_hook(hook_id) -> bool: Also delete associated hook_log entries.
     - record_hook_triggered(hook_id, status)

     Webhook configs:
     - create_webhook(name, event_name, url, secret, retry_count, timeout_seconds, enabled) -> dict
     - get_webhooks(event_name, enabled_only) -> list[dict]
     - get_webhook(webhook_id) -> Optional[dict]
     - update_webhook(webhook_id, **kwargs) -> bool
     - delete_webhook(webhook_id) -> bool: Also delete associated hook_log entries.
     - record_webhook_triggered(webhook_id, status_code, error)

     Hook logs:
     - create_hook_log(hook_id, webhook_id, event_name, hook_type, success, exit_code, status_code, stdout, stderr, error, duration_ms) -> dict
     - get_hook_logs(limit, offset, hook_id, webhook_id) -> list[dict]
     - get_hook_log_count(hook_id, webhook_id) -> int
     - clear_hook_logs() -> int
     - clear_old_hook_logs(days) -> int

   - Read db/hooks.py carefully. The record_hook_triggered and record_webhook_triggered functions update last_triggered_at, last_status/last_status_code, trigger_count, and consecutive_failures.
   - Helpers: _row_to_hook, _row_to_webhook, _row_to_log.

3. Create backend/db/repositories/standalone.py (replacing 408-line db/standalone.py):
   - class StandaloneRepository(BaseRepository):
     Watched folders:
     - create_watched_folder(path, label, media_type) -> dict
     - get_watched_folders(enabled_only) -> list[dict]
     - get_watched_folder(folder_id) -> Optional[dict]
     - update_watched_folder(folder_id, **kwargs) -> bool
     - delete_watched_folder(folder_id) -> bool
     - update_last_scan(folder_id)

     Standalone series:
     - upsert_standalone_series(title, year, folder_path, tmdb_id, tvdb_id, anilist_id, imdb_id, poster_url, is_anime, episode_count, season_count, metadata_source) -> dict: Upsert on folder_path.
     - get_standalone_series(series_id) -> Optional[dict]
     - get_all_standalone_series() -> list[dict]
     - delete_standalone_series(series_id) -> bool
     - get_standalone_series_by_folder(folder_path) -> Optional[dict]

     Standalone movies:
     - upsert_standalone_movie(title, year, file_path, tmdb_id, imdb_id, poster_url, metadata_source) -> dict: Upsert on file_path.
     - get_standalone_movie(movie_id) -> Optional[dict]
     - get_all_standalone_movies() -> list[dict]
     - delete_standalone_movie(movie_id) -> bool
     - get_standalone_movie_by_path(file_path) -> Optional[dict]

     Metadata cache:
     - get_metadata_cache(cache_key) -> Optional[dict]: Check expiry.
     - save_metadata_cache(cache_key, provider, response_json, ttl_days)
     - clear_expired_metadata_cache() -> int

     AniDB mappings:
     - get_anidb_mapping(tvdb_id) -> Optional[dict]: Update last_used on read.
     - save_anidb_mapping(tvdb_id, anidb_id, series_title)
     - clear_old_anidb_mappings(ttl_days) -> int

   - Read db/standalone.py for exact function signatures and return types.
   - Helpers: _row_to_series, _row_to_movie, _row_to_folder.

4. Update backend/db/repositories/__init__.py to re-export all 14 repository classes (base + 13 domain repos from Plans 02 and 03).
  </action>
  <verify>
Run `cd Z:/CC/Sublarr/backend && python -c "from db.repositories.providers import ProviderRepository; print('ProviderRepository OK')"`.
Run `cd Z:/CC/Sublarr/backend && python -c "from db.repositories.hooks import HookRepository; print('HookRepository OK')"`.
Run `cd Z:/CC/Sublarr/backend && python -c "from db.repositories.standalone import StandaloneRepository; print('StandaloneRepository OK')"`.
Run `cd Z:/CC/Sublarr/backend && python -c "from db.repositories import *; print('All 14 repositories importable')"`.
  </verify>
  <done>All 6 complex repository classes created. Provider weighted average, wanted upsert, hook cascading, standalone upsert logic all preserved. Full repository layer complete -- all 14 repositories cover all 15 db/ domain modules.</done>
</task>

</tasks>

<verification>
1. All 14 repository classes importable from db.repositories
2. Each repository class has methods matching its db/ module counterpart
3. No raw SQL (sqlite3) in any repository -- only SQLAlchemy ORM operations
4. Return types match existing db/ functions (dicts, not model instances)
5. No existing code changed (additive alongside existing db/ modules)
</verification>

<success_criteria>
- Complete repository layer for all database operations
- Complex logic preserved: weighted averages (providers), multi-column upserts (wanted), cascade deletes (profiles/hooks), JSON merge (daily_stats)
- All repositories use SQLAlchemy 2.0 query syntax
- Ready for Plan 05 to swap imports from db.* to db.repositories.*
</success_criteria>

<output>
After completion, create `.planning/phases/10-performance-scalability/10-03-SUMMARY.md`
</output>
