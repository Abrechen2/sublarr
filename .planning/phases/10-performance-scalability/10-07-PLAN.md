---
phase: 10-performance-scalability
plan: 07
type: execute
wave: 4
depends_on: ["10-05"]
files_modified:
  - backend/metrics.py
  - monitoring/grafana/dashboards/sublarr-overview.json
  - monitoring/grafana/dashboards/sublarr-database.json
  - monitoring/grafana/provisioning/dashboards.yml
  - monitoring/grafana/provisioning/datasources.yml
  - monitoring/README.md
autonomous: true

must_haves:
  truths:
    - "Extended Prometheus metrics cover HTTP requests, DB queries, cache hits/misses, Redis status, queue depth"
    - "Grafana dashboards are pre-built and ship as JSON provisioning files"
    - "Monitoring setup guide explains how to connect Prometheus and Grafana to Sublarr"
  artifacts:
    - path: "backend/metrics.py"
      provides: "Extended metrics: HTTP duration, DB query duration, cache stats, Redis status, pool stats"
      contains: "HTTP_REQUEST_DURATION"
    - path: "monitoring/grafana/dashboards/sublarr-overview.json"
      provides: "Main Grafana dashboard with translations, providers, queue, system panels"
      min_lines: 100
    - path: "monitoring/grafana/dashboards/sublarr-database.json"
      provides: "Database-focused Grafana dashboard with pool stats, query times, cache metrics"
      min_lines: 80
    - path: "monitoring/grafana/provisioning/dashboards.yml"
      provides: "Grafana dashboard provisioning config pointing to JSON files"
      contains: "sublarr"
  key_links:
    - from: "backend/metrics.py"
      to: "backend/app.py"
      via: "metrics collection called during request lifecycle"
      pattern: "collect_.*_metrics"
    - from: "monitoring/grafana/dashboards/sublarr-overview.json"
      to: "backend/metrics.py"
      via: "PromQL queries referencing sublarr_* metric names"
      pattern: "sublarr_"
---

<objective>
Extend Prometheus metrics with HTTP request timing, database query tracking, cache hit/miss rates, Redis connection status, and connection pool stats. Create pre-built Grafana dashboard JSON files that visualize these metrics out of the box.

Purpose: Users running Sublarr at scale need observability into performance bottlenecks. Extended metrics (PERF-07) and Grafana dashboards (PERF-08) provide this without requiring users to build their own monitoring. The metrics also validate that the SQLAlchemy migration didn't introduce performance regressions.

Output: Extended metrics.py with 10+ new metric definitions, 2 Grafana dashboard JSON files with provisioning config.
</objective>

<execution_context>
@C:/Users/Dennis Wittke/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Dennis Wittke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-performance-scalability/10-RESEARCH.md
@.planning/phases/10-performance-scalability/10-05-SUMMARY.md
@backend/metrics.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extended Prometheus metrics</name>
  <files>backend/metrics.py</files>
  <action>
Add the following new metric definitions to backend/metrics.py (inside the `if METRICS_AVAILABLE:` block):

1. HTTP Request Metrics:
   ```python
   HTTP_REQUEST_DURATION = Histogram(
       "sublarr_http_request_duration_seconds",
       "HTTP request duration",
       ["method", "endpoint", "status"],
       buckets=(0.01, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0),
   )
   HTTP_REQUEST_TOTAL = Counter(
       "sublarr_http_requests_total",
       "Total HTTP requests",
       ["method", "endpoint", "status"],
   )
   HTTP_REQUESTS_IN_PROGRESS = Gauge(
       "sublarr_http_requests_in_progress",
       "HTTP requests currently being processed",
   )
   ```

2. Database Metrics:
   ```python
   DB_QUERY_DURATION = Histogram(
       "sublarr_db_query_duration_seconds",
       "Database query duration",
       ["operation"],  # select, insert, update, delete
       buckets=(0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5),
   )
   DB_QUERY_TOTAL = Counter(
       "sublarr_db_queries_total",
       "Total database queries",
       ["operation"],
   )
   DB_POOL_SIZE = Gauge(
       "sublarr_db_pool_size",
       "Database connection pool current size",
   )
   DB_POOL_CHECKED_OUT = Gauge(
       "sublarr_db_pool_checked_out",
       "Database connections currently checked out",
   )
   DB_POOL_OVERFLOW = Gauge(
       "sublarr_db_pool_overflow",
       "Database connection pool overflow count",
   )
   DB_BACKEND = Info(
       "sublarr_db_backend",
       "Database backend information",
   )
   ```

3. Cache Metrics:
   ```python
   CACHE_HITS_TOTAL = Counter(
       "sublarr_cache_hits_total",
       "Cache hit counter",
       ["backend"],  # redis, memory
   )
   CACHE_MISSES_TOTAL = Counter(
       "sublarr_cache_misses_total",
       "Cache miss counter",
       ["backend"],
   )
   CACHE_SIZE = Gauge(
       "sublarr_cache_size",
       "Number of items in cache",
       ["backend"],
   )
   ```

4. Redis Metrics:
   ```python
   REDIS_CONNECTED = Gauge(
       "sublarr_redis_connected",
       "Redis connection status (1=connected, 0=disconnected)",
   )
   REDIS_MEMORY_USED = Gauge(
       "sublarr_redis_memory_used_bytes",
       "Redis memory usage in bytes",
   )
   ```

5. Queue Metrics:
   ```python
   QUEUE_SIZE = Gauge(
       "sublarr_queue_size",
       "Number of jobs in queue",
       ["backend"],  # rq, memory
   )
   QUEUE_ACTIVE = Gauge(
       "sublarr_queue_active_jobs",
       "Number of actively running jobs",
       ["backend"],
   )
   QUEUE_FAILED = Gauge(
       "sublarr_queue_failed_jobs",
       "Number of failed jobs",
       ["backend"],
   )
   ```

6. Add collection functions:

   ```python
   def collect_db_pool_metrics():
       """Update database connection pool gauges."""
       if not METRICS_AVAILABLE:
           return
       try:
           from extensions import db
           pool = db.engine.pool
           # NullPool (SQLite) doesn't have these attrs
           if hasattr(pool, 'size'):
               DB_POOL_SIZE.set(pool.size())
               DB_POOL_CHECKED_OUT.set(pool.checkedout())
               DB_POOL_OVERFLOW.set(pool.overflow())
           DB_BACKEND.info({"dialect": db.engine.dialect.name})
       except Exception as exc:
           logger.debug("Failed to collect DB pool metrics: %s", exc)

   def collect_cache_metrics():
       """Update cache gauges from app cache backend."""
       if not METRICS_AVAILABLE:
           return
       try:
           from flask import current_app
           cache = getattr(current_app, 'cache_backend', None)
           if cache:
               stats = cache.get_stats()
               backend = stats.get("backend", "unknown")
               CACHE_HITS_TOTAL.labels(backend=backend)  # counter, no set
               CACHE_SIZE.labels(backend=backend).set(stats.get("size", 0))
       except Exception:
           pass

   def collect_redis_metrics():
       """Update Redis connection and memory gauges."""
       if not METRICS_AVAILABLE:
           return
       try:
           from flask import current_app
           cache = getattr(current_app, 'cache_backend', None)
           if cache and hasattr(cache, 'redis'):
               REDIS_CONNECTED.set(1)
               info = cache.redis.info("memory")
               REDIS_MEMORY_USED.set(info.get("used_memory", 0))
           else:
               REDIS_CONNECTED.set(0)
       except Exception:
           REDIS_CONNECTED.set(0)

   def collect_queue_job_metrics():
       """Update queue depth gauges from app job queue."""
       if not METRICS_AVAILABLE:
           return
       try:
           from flask import current_app
           queue = getattr(current_app, 'job_queue', None)
           if queue:
               info = queue.get_backend_info()
               backend = info.get("type", "unknown")
               QUEUE_SIZE.labels(backend=backend).set(queue.get_queue_length())
               QUEUE_ACTIVE.labels(backend=backend).set(len(queue.get_active_jobs()))
               QUEUE_FAILED.labels(backend=backend).set(len(queue.get_failed_jobs(100)))
       except Exception:
           pass
   ```

7. Add helper functions for instrumentation (used by app.py or middleware):
   ```python
   def record_http_request(method, endpoint, status, duration):
       """Record an HTTP request metric."""
       if not METRICS_AVAILABLE:
           return
       HTTP_REQUEST_DURATION.labels(method=method, endpoint=endpoint, status=status).observe(duration)
       HTTP_REQUEST_TOTAL.labels(method=method, endpoint=endpoint, status=status).inc()

   def record_db_query(operation, duration):
       """Record a database query metric."""
       if not METRICS_AVAILABLE:
           return
       DB_QUERY_DURATION.labels(operation=operation).observe(duration)
       DB_QUERY_TOTAL.labels(operation=operation).inc()
   ```

8. Update generate_metrics() to call new collection functions:
   ```python
   def generate_metrics(db_path: str) -> tuple[bytes, str]:
       # ... existing calls ...
       collect_db_pool_metrics()
       collect_cache_metrics()
       collect_redis_metrics()
       collect_queue_job_metrics()
       return generate_latest(REGISTRY), CONTENT_TYPE_LATEST
   ```

Keep all existing metrics unchanged. New metrics are purely additive.
  </action>
  <verify>
Run `cd Z:/CC/Sublarr/backend && python -c "from metrics import METRICS_AVAILABLE; print('Metrics available:', METRICS_AVAILABLE)"`.
Run `cd Z:/CC/Sublarr/backend && python -c "from metrics import HTTP_REQUEST_DURATION, DB_QUERY_DURATION, CACHE_HITS_TOTAL, REDIS_CONNECTED, QUEUE_SIZE; print('All new metrics defined')"` (only works if prometheus_client installed).
  </verify>
  <done>10+ new Prometheus metrics covering HTTP requests, DB queries, cache, Redis, and queue. Collection functions gather data from SQLAlchemy pool, cache backend, and queue backend. Helper functions for request/query instrumentation.</done>
</task>

<task type="auto">
  <name>Task 2: Grafana dashboard JSON files and provisioning</name>
  <files>
    monitoring/grafana/dashboards/sublarr-overview.json
    monitoring/grafana/dashboards/sublarr-database.json
    monitoring/grafana/provisioning/dashboards.yml
    monitoring/grafana/provisioning/datasources.yml
    monitoring/README.md
  </files>
  <action>
1. Create monitoring/grafana/provisioning/datasources.yml:
   ```yaml
   apiVersion: 1
   datasources:
     - name: Prometheus
       type: prometheus
       access: proxy
       url: http://prometheus:9090
       isDefault: true
       editable: false
   ```

2. Create monitoring/grafana/provisioning/dashboards.yml:
   ```yaml
   apiVersion: 1
   providers:
     - name: "Sublarr"
       orgId: 1
       folder: "Sublarr"
       type: file
       disableDeletion: false
       editable: true
       options:
         path: /var/lib/grafana/dashboards
         foldersFromFilesStructure: false
   ```

3. Create monitoring/grafana/dashboards/sublarr-overview.json:
   Grafana dashboard JSON with the following panels in a 2-column layout:

   Row 1 - System Overview:
   - Panel: "Translation Operations" (graph, timeseries)
     - Query: rate(sublarr_translation_total[5m]) by status
     - Shows success/failed/skipped translation rates
   - Panel: "Active Jobs" (stat)
     - Query: sublarr_job_queue_size + sublarr_queue_active_jobs
     - Shows current job queue depth

   Row 2 - Provider Performance:
   - Panel: "Provider Search Rate" (timeseries)
     - Query: rate(sublarr_provider_search_total[5m]) by provider, status
   - Panel: "Provider Download Rate" (timeseries)
     - Query: rate(sublarr_provider_download_total[5m]) by provider

   Row 3 - HTTP API:
   - Panel: "Request Duration (p95)" (timeseries)
     - Query: histogram_quantile(0.95, rate(sublarr_http_request_duration_seconds_bucket[5m]))
   - Panel: "Request Rate" (timeseries)
     - Query: rate(sublarr_http_requests_total[5m]) by status

   Row 4 - System Resources:
   - Panel: "CPU Usage" (gauge)
     - Query: sublarr_cpu_usage_percent
   - Panel: "Memory Usage" (stat)
     - Query: sublarr_memory_usage_bytes
   - Panel: "Database Size" (stat)
     - Query: sublarr_database_size_bytes
   - Panel: "Circuit Breakers" (table)
     - Query: sublarr_circuit_breaker_state

   Dashboard settings: auto-refresh 30s, time range last 1h, dark theme.
   Use uid: "sublarr-overview" for stable linking.

4. Create monitoring/grafana/dashboards/sublarr-database.json:
   Grafana dashboard JSON with database-focused panels:

   Row 1 - Connection Pool:
   - Panel: "Pool Status" (stat, 3 values)
     - Queries: sublarr_db_pool_size, sublarr_db_pool_checked_out, sublarr_db_pool_overflow
   - Panel: "Pool Utilization" (gauge)
     - Query: sublarr_db_pool_checked_out / sublarr_db_pool_size * 100

   Row 2 - Query Performance:
   - Panel: "Query Duration (p95)" (timeseries)
     - Query: histogram_quantile(0.95, rate(sublarr_db_query_duration_seconds_bucket[5m])) by operation
   - Panel: "Query Rate" (timeseries)
     - Query: rate(sublarr_db_queries_total[5m]) by operation

   Row 3 - Cache:
   - Panel: "Cache Hit Rate" (gauge)
     - Query: rate(sublarr_cache_hits_total[5m]) / (rate(sublarr_cache_hits_total[5m]) + rate(sublarr_cache_misses_total[5m])) * 100
   - Panel: "Cache Size" (stat)
     - Query: sublarr_cache_size

   Row 4 - Redis (collapsed, shown only when Redis active):
   - Panel: "Redis Status" (stat)
     - Query: sublarr_redis_connected
   - Panel: "Redis Memory" (stat)
     - Query: sublarr_redis_memory_used_bytes
   - Panel: "Queue Depth" (timeseries)
     - Queries: sublarr_queue_size, sublarr_queue_active_jobs, sublarr_queue_failed_jobs

   Dashboard settings: auto-refresh 30s, uid: "sublarr-database".

5. Create monitoring/README.md:
   Brief setup guide:
   - How to add Prometheus scrape target for Sublarr (/metrics endpoint)
   - How to provision Grafana dashboards (mount the monitoring/ directory)
   - Docker compose snippet showing Prometheus + Grafana alongside Sublarr
   - Screenshot descriptions of each dashboard
   - Metric reference table listing all sublarr_* metrics
  </action>
  <verify>
Verify JSON is valid: `cd Z:/CC/Sublarr && python -c "import json; json.load(open('monitoring/grafana/dashboards/sublarr-overview.json')); print('Overview dashboard valid JSON')"`.
Verify JSON is valid: `cd Z:/CC/Sublarr && python -c "import json; json.load(open('monitoring/grafana/dashboards/sublarr-database.json')); print('Database dashboard valid JSON')"`.
Verify YAML is valid: `cd Z:/CC/Sublarr && python -c "import yaml; yaml.safe_load(open('monitoring/grafana/provisioning/dashboards.yml')); print('Provisioning YAML valid')"`.
  </verify>
  <done>Two Grafana dashboards (overview + database) with comprehensive panels for all Sublarr metrics. Provisioning config for auto-import. README with setup instructions and metric reference.</done>
</task>

</tasks>

<verification>
1. All new metrics defined in metrics.py (import check)
2. generate_metrics() collects new metrics alongside existing ones
3. Both Grafana dashboards are valid JSON
4. Dashboard PromQL queries reference correct metric names
5. Provisioning YAML is valid
6. monitoring/README.md provides clear setup instructions
</verification>

<success_criteria>
- 10+ new Prometheus metrics covering HTTP, DB, cache, Redis, queue
- 2 pre-built Grafana dashboards importable via provisioning
- Monitoring setup documented
- All metrics gracefully no-op when prometheus_client not installed
</success_criteria>

<output>
After completion, create `.planning/phases/10-performance-scalability/10-07-SUMMARY.md`
</output>
