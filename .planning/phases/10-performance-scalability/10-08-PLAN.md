---
phase: 10-performance-scalability
plan: 08
type: execute
wave: 4
depends_on: ["10-05"]
files_modified:
  - backend/providers/__init__.py
  - backend/db/repositories/cache.py
  - backend/translator.py
  - backend/wanted_search.py
autonomous: true

must_haves:
  truths:
    - "Provider search results are cached via app.cache_backend (Redis when configured, memory otherwise) instead of only using the SQLite provider_cache table"
    - "Translation and wanted search jobs use app.job_queue for execution (RQ when configured, ThreadPoolExecutor otherwise) instead of only using daemon threads"
    - "Existing provider cache in db.repositories.cache still works as the persistent record; app.cache_backend adds a fast lookup layer on top"
  artifacts:
    - path: "backend/providers/__init__.py"
      provides: "ProviderManager uses app.cache_backend for fast cache lookups before falling back to DB cache"
      contains: "cache_backend"
    - path: "backend/translator.py"
      provides: "Translation jobs submitted via app.job_queue when available"
      contains: "job_queue"
    - path: "backend/wanted_search.py"
      provides: "Wanted search jobs submitted via app.job_queue when available"
      contains: "job_queue"
  key_links:
    - from: "backend/providers/__init__.py"
      to: "backend/cache/__init__.py"
      via: "app.cache_backend.get/set for provider result caching"
      pattern: "cache_backend\\.(get|set)"
    - from: "backend/translator.py"
      to: "backend/queue/__init__.py"
      via: "app.job_queue.enqueue for translation jobs"
      pattern: "job_queue\\.enqueue"
    - from: "backend/wanted_search.py"
      to: "backend/queue/__init__.py"
      via: "app.job_queue.enqueue for wanted search jobs"
      pattern: "job_queue\\.enqueue"
---

<objective>
Wire the cache and job queue backends created in Plan 04 (and stored on app in Plan 05) into the actual application code that should USE them. Without this plan, app.cache_backend and app.job_queue exist but are never called by any business logic.

Purpose: This is the "last mile" integration that makes Redis actually useful. Provider search results get a fast Redis cache layer (falling back to memory cache, with the SQLite/PostgreSQL provider_cache table remaining as the persistent record). Translation and wanted search jobs get routed through the job queue abstraction (RQ for persistent jobs when Redis is configured, ThreadPoolExecutor when not).

Output: Provider cache reads/writes use app.cache_backend as a fast layer. Translation/wanted jobs are submitted via app.job_queue.
</objective>

<execution_context>
@C:/Users/Dennis Wittke/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Dennis Wittke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-performance-scalability/10-RESEARCH.md
@.planning/phases/10-performance-scalability/10-04-SUMMARY.md
@.planning/phases/10-performance-scalability/10-05-SUMMARY.md
@backend/providers/__init__.py
@backend/translator.py
@backend/wanted_search.py
@backend/db/repositories/cache.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Wire cache backend into provider search caching</name>
  <files>
    backend/providers/__init__.py
    backend/db/repositories/cache.py
  </files>
  <action>
The ProviderManager in backend/providers/__init__.py currently uses `db.cache.get_cached_results()` and `db.cache.save_cache_results()` which go through the CacheRepository to the provider_cache SQLite table. We add a fast cache layer on top using app.cache_backend.

1. Update backend/providers/__init__.py ProviderManager:

   In the search method (where provider results are fetched), add a two-tier cache lookup:

   a. BEFORE checking the DB cache, check app.cache_backend:
      ```python
      from flask import current_app
      import json

      cache_key = f"provider:{provider_name}:{query_hash}"
      cache = getattr(current_app, 'cache_backend', None)
      if cache:
          cached = cache.get(cache_key)
          if cached:
              return json.loads(cached)  # Fast Redis/memory hit
      ```

   b. If app.cache_backend misses, fall through to existing DB cache check (`db.cache.get_cached_results()`).

   c. After a successful provider search, write to BOTH caches:
      - app.cache_backend.set(cache_key, json.dumps(results), ttl_seconds=ttl_minutes*60)
      - db.cache.save_cache_results(...) (existing DB persistence -- keeps the audit trail)

   d. When cache is cleared/invalidated, clear from BOTH:
      - app.cache_backend.clear(prefix="provider:")
      - db.cache.clear_provider_cache(...) (existing)

   IMPORTANT: The app.cache_backend is an OPTIONAL fast layer. If it is None (e.g., outside Flask context, or during testing), skip it and use DB cache only. The getattr pattern with None check handles this gracefully.

   IMPORTANT: Do NOT replace the DB cache entirely. The provider_cache table serves as the persistent audit trail and is visible in the Provider Stats UI. The app.cache_backend is a transparent acceleration layer.

2. Update backend/db/repositories/cache.py:

   Add a helper method `invalidate_app_cache(prefix)` that calls `current_app.cache_backend.clear(prefix)` if available. This ensures that when the CacheRepository clears DB cache, the app cache is also invalidated.

   Add try/except around app cache operations so a Redis failure never blocks provider search functionality.
  </action>
  <verify>
Run `cd Z:/CC/Sublarr/backend && python -c "
from app import create_app
app = create_app(testing=True)
with app.app_context():
    # Verify cache_backend is initialized
    cache = app.cache_backend
    assert cache is not None
    print(f'Cache backend type: {cache.get_stats()[\"backend\"]}')

    # Verify provider imports still work
    from providers import search_subtitles
    print('Provider search importable')

    # Verify cache is used in provider code
    import providers as prov_module
    source = open(prov_module.__file__).read()
    assert 'cache_backend' in source, 'cache_backend not wired into providers/__init__.py'
    print('cache_backend wired into provider code: OK')
"`.
  </verify>
  <done>Provider search results use app.cache_backend as fast lookup layer (Redis or memory) with DB provider_cache as persistent fallback. Cache invalidation clears both layers.</done>
</task>

<task type="auto">
  <name>Task 2: Wire job queue into translator and wanted search</name>
  <files>
    backend/translator.py
    backend/wanted_search.py
  </files>
  <action>
The translator.py and wanted_search.py currently use daemon threads or direct function calls for background work. We route these through app.job_queue so they benefit from RQ persistence when Redis is configured.

1. Update backend/translator.py:

   The translate function (or its background entry point) currently runs in a daemon thread or is called directly. Modify the code that SUBMITS translation jobs (not the translation logic itself) to use app.job_queue:

   ```python
   from flask import current_app

   def submit_translation_job(file_path, **kwargs):
       """Submit a translation job via the job queue."""
       queue = getattr(current_app, 'job_queue', None)
       if queue:
           job_id = queue.enqueue(
               _do_translate,  # The actual translation function
               file_path,
               job_id=kwargs.get('job_id'),
               **kwargs
           )
           return job_id
       else:
           # Fallback: direct execution (should not happen in normal operation)
           return _do_translate(file_path, **kwargs)
   ```

   IMPORTANT: The translation function itself (_do_translate or translate_file) is NOT changed. Only the submission mechanism changes. The function still receives the same arguments and produces the same results.

   IMPORTANT: For RQ, the function being enqueued must be importable by the worker process (top-level function, not a lambda or closure). Ensure _do_translate is a module-level function.

   IMPORTANT: If the existing code uses `threading.Thread(target=...)` for background translation, replace that pattern with `queue.enqueue(...)`. Keep the synchronous path (`/translate/sync`) as a direct call (no queue).

2. Update backend/wanted_search.py:

   Similarly, the wanted search functions that process batches of wanted items in the background should use app.job_queue:

   ```python
   def submit_wanted_search(item_id):
       """Submit a wanted search job via the job queue."""
       queue = getattr(current_app, 'job_queue', None)
       if queue:
           return queue.enqueue(
               _process_wanted_item,
               item_id,
               job_id=f"wanted-{item_id}"
           )
       else:
           return _process_wanted_item(item_id)
   ```

   For batch operations, submit each item as a separate job:
   ```python
   def submit_wanted_batch_search(item_ids):
       queue = getattr(current_app, 'job_queue', None)
       if queue:
           return [queue.enqueue(_process_wanted_item, iid, job_id=f"wanted-{iid}") for iid in item_ids]
       else:
           return [_process_wanted_item(iid) for iid in item_ids]
   ```

   IMPORTANT: Same rules as translator -- only change submission, not the search logic itself. The _process_wanted_item function is unchanged.

   IMPORTANT: For the MemoryJobQueue fallback, this behaves identically to the current ThreadPoolExecutor pattern. For RQ, jobs survive container restarts and can be monitored via the queue API.

3. Ensure both modules handle the case where current_app is not available (e.g., during startup or in background threads that lose Flask context). Use getattr with None fallback.
  </action>
  <verify>
Run `cd Z:/CC/Sublarr/backend && python -c "
from app import create_app
app = create_app(testing=True)
with app.app_context():
    # Verify job_queue is initialized
    queue = app.job_queue
    assert queue is not None
    info = queue.get_backend_info()
    print(f'Queue backend type: {info[\"type\"]}')

    # Verify translator is wired
    import translator
    source = open(translator.__file__).read()
    assert 'job_queue' in source, 'job_queue not wired into translator.py'
    print('job_queue wired into translator: OK')

    # Verify wanted_search is wired
    import wanted_search
    source = open(wanted_search.__file__).read()
    assert 'job_queue' in source, 'job_queue not wired into wanted_search.py'
    print('job_queue wired into wanted_search: OK')
"`.
Run existing tests: `cd Z:/CC/Sublarr/backend && python -m pytest tests/ -x --timeout=60 -q`.
  </verify>
  <done>Translation and wanted search jobs are submitted via app.job_queue (RQ when Redis configured, ThreadPoolExecutor when not). Job submission is the only change; business logic is untouched. Backward compatible: without Redis, behavior is identical to before.</done>
</task>

</tasks>

<verification>
1. Provider cache uses app.cache_backend for fast lookups: grep for 'cache_backend' in providers/__init__.py
2. Translation jobs use app.job_queue: grep for 'job_queue' in translator.py
3. Wanted search jobs use app.job_queue: grep for 'job_queue' in wanted_search.py
4. All fallbacks work without Redis: app starts and operates with default config
5. Existing tests pass: `python -m pytest tests/ -x`
</verification>

<success_criteria>
- Provider search results cached in app.cache_backend (fast layer) alongside existing DB cache (persistent layer)
- Translation jobs submitted via app.job_queue
- Wanted search jobs submitted via app.job_queue
- All backends fall back gracefully when Redis is not configured
- No behavioral changes for users running without Redis
</success_criteria>

<output>
After completion, create `.planning/phases/10-performance-scalability/10-08-SUMMARY.md`
</output>
