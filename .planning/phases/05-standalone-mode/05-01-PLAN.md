---
phase: 05-standalone-mode
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/db/__init__.py
  - backend/db/standalone.py
  - backend/config.py
  - backend/standalone/__init__.py
  - backend/standalone/parser.py
autonomous: true

must_haves:
  truths:
    - "Standalone DB tables exist and accept inserts for watched_folders, standalone_series, standalone_movies, metadata_cache"
    - "Media filenames are parsed into structured metadata (title, season, episode, year, release_group, type)"
    - "Anime filenames with absolute numbering are correctly parsed using episode_prefer_number"
    - "Config settings for standalone mode are accessible via get_settings()"
  artifacts:
    - path: "backend/db/__init__.py"
      provides: "Schema DDL for 4 new tables + migrations for wanted_items standalone columns"
      contains: "watched_folders"
    - path: "backend/db/standalone.py"
      provides: "CRUD operations for watched_folders, standalone_series, standalone_movies, metadata_cache"
      exports: ["upsert_watched_folder", "get_watched_folders", "upsert_standalone_series", "get_standalone_series", "upsert_standalone_movie"]
    - path: "backend/config.py"
      provides: "Standalone config fields (standalone_enabled, tmdb_api_key, tvdb_api_key, etc.)"
      contains: "standalone_enabled"
    - path: "backend/standalone/parser.py"
      provides: "MediaFileParser wrapping guessit with anime detection"
      exports: ["parse_media_file", "detect_anime_indicators"]
  key_links:
    - from: "backend/db/standalone.py"
      to: "backend/db/__init__.py"
      via: "get_db and _db_lock imports"
      pattern: "from db import get_db, _db_lock"
    - from: "backend/standalone/parser.py"
      to: "guessit"
      via: "guessit library call"
      pattern: "from guessit import guessit"
---

<objective>
Create the database schema, config settings, and media file parser for standalone mode.

Purpose: Establish the data foundation (4 new DB tables, config fields, wanted_items columns) and the filename parsing layer that all other standalone plans depend on.
Output: `backend/db/standalone.py`, `backend/standalone/parser.py`, updated schema in `backend/db/__init__.py`, new config fields in `backend/config.py`.
</objective>

<execution_context>
@C:/Users/Dennis Wittke/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Dennis Wittke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-standalone-mode/05-RESEARCH.md
@backend/db/__init__.py
@backend/db/wanted.py
@backend/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Database schema and operations for standalone mode</name>
  <files>backend/db/__init__.py, backend/db/standalone.py</files>
  <action>
  1. In `backend/db/__init__.py` SCHEMA string, add 4 new CREATE TABLE statements (AFTER the existing whisper_jobs table):

  ```sql
  CREATE TABLE IF NOT EXISTS watched_folders (
      id INTEGER PRIMARY KEY AUTOINCREMENT,
      path TEXT NOT NULL UNIQUE,
      label TEXT DEFAULT '',
      media_type TEXT DEFAULT 'auto',  -- 'auto', 'tv', 'movie'
      enabled INTEGER DEFAULT 1,
      last_scan_at TEXT DEFAULT '',
      created_at TEXT NOT NULL DEFAULT (datetime('now')),
      updated_at TEXT NOT NULL DEFAULT (datetime('now'))
  );

  CREATE TABLE IF NOT EXISTS standalone_series (
      id INTEGER PRIMARY KEY AUTOINCREMENT,
      title TEXT NOT NULL,
      year INTEGER,
      folder_path TEXT NOT NULL,
      tmdb_id INTEGER,
      tvdb_id INTEGER,
      anilist_id INTEGER,
      imdb_id TEXT DEFAULT '',
      poster_url TEXT DEFAULT '',
      is_anime INTEGER DEFAULT 0,
      episode_count INTEGER DEFAULT 0,
      season_count INTEGER DEFAULT 0,
      metadata_source TEXT DEFAULT '',
      created_at TEXT NOT NULL DEFAULT (datetime('now')),
      updated_at TEXT NOT NULL DEFAULT (datetime('now')),
      UNIQUE(folder_path)
  );
  CREATE INDEX IF NOT EXISTS idx_standalone_series_tmdb ON standalone_series(tmdb_id);
  CREATE INDEX IF NOT EXISTS idx_standalone_series_anilist ON standalone_series(anilist_id);

  CREATE TABLE IF NOT EXISTS standalone_movies (
      id INTEGER PRIMARY KEY AUTOINCREMENT,
      title TEXT NOT NULL,
      year INTEGER,
      file_path TEXT NOT NULL UNIQUE,
      tmdb_id INTEGER,
      imdb_id TEXT DEFAULT '',
      poster_url TEXT DEFAULT '',
      metadata_source TEXT DEFAULT '',
      created_at TEXT NOT NULL DEFAULT (datetime('now')),
      updated_at TEXT NOT NULL DEFAULT (datetime('now'))
  );
  CREATE INDEX IF NOT EXISTS idx_standalone_movies_tmdb ON standalone_movies(tmdb_id);

  CREATE TABLE IF NOT EXISTS metadata_cache (
      cache_key TEXT PRIMARY KEY,
      provider TEXT NOT NULL,
      response_json TEXT NOT NULL,
      cached_at TEXT NOT NULL,
      expires_at TEXT NOT NULL
  );
  CREATE INDEX IF NOT EXISTS idx_metadata_cache_expires ON metadata_cache(expires_at);
  ```

  2. In `_run_migrations()`, add migration for existing DBs -- add `standalone_series_id` and `standalone_movie_id` columns to wanted_items:
  ```python
  # Add standalone columns to wanted_items
  cursor = conn.execute("PRAGMA table_info(wanted_items)")
  wi_columns = {row[1] for row in cursor.fetchall()}
  if "standalone_series_id" not in wi_columns:
      conn.execute("ALTER TABLE wanted_items ADD COLUMN standalone_series_id INTEGER")
  if "standalone_movie_id" not in wi_columns:
      conn.execute("ALTER TABLE wanted_items ADD COLUMN standalone_movie_id INTEGER")
  ```

  Also add migration for creating the 4 new tables on existing DBs (same try/except pattern as glossary_entries):
  ```python
  for table_name in ["watched_folders", "standalone_series", "standalone_movies", "metadata_cache"]:
      try:
          conn.execute(f"SELECT 1 FROM {table_name} LIMIT 1")
      except sqlite3.OperationalError:
          # Tables will be created by executescript(SCHEMA) on fresh DBs
          # For existing DBs, run the CREATE TABLE statements
          conn.executescript(SCHEMA)
          break
  ```

  3. Create `backend/db/standalone.py` with CRUD operations following the exact pattern of `db/wanted.py` (imports get_db and _db_lock from db):

  Functions to implement:
  - `upsert_watched_folder(path, label="", media_type="auto", enabled=True) -> int` -- insert or update by path
  - `get_watched_folders(enabled_only=True) -> list[dict]` -- return all (or enabled-only) folders
  - `get_watched_folder(folder_id) -> dict | None`
  - `delete_watched_folder(folder_id) -> bool`
  - `upsert_standalone_series(title, folder_path, year=None, tmdb_id=None, tvdb_id=None, anilist_id=None, imdb_id="", poster_url="", is_anime=False, episode_count=0, season_count=0, metadata_source="") -> int` -- insert or update by folder_path
  - `get_standalone_series(series_id=None) -> dict | list[dict]` -- single by id or all
  - `get_standalone_series_by_folder(folder_path) -> dict | None`
  - `delete_standalone_series(series_id) -> bool`
  - `upsert_standalone_movie(title, file_path, year=None, tmdb_id=None, imdb_id="", poster_url="", metadata_source="") -> int`
  - `get_standalone_movies(movie_id=None) -> dict | list[dict]`
  - `delete_standalone_movie(movie_id) -> bool`
  - `cache_metadata(cache_key, provider, response_json, ttl_days=30) -> None` -- insert/replace with TTL
  - `get_cached_metadata(cache_key) -> dict | None` -- return None if expired
  - `cleanup_expired_cache() -> int` -- delete expired entries, return count

  Every function uses `with _db_lock:` for thread safety. Return dicts (not Row objects) using `dict(row)` pattern.
  </action>
  <verify>
  ```bash
  cd backend && python -c "
from db import init_db, get_db, _db_lock
init_db()
db = get_db()
# Verify tables exist
for t in ['watched_folders', 'standalone_series', 'standalone_movies', 'metadata_cache']:
    count = db.execute(f'SELECT COUNT(*) FROM {t}').fetchone()[0]
    print(f'{t}: exists (count={count})')
# Verify wanted_items columns
cols = {r[1] for r in db.execute('PRAGMA table_info(wanted_items)').fetchall()}
assert 'standalone_series_id' in cols, 'Missing standalone_series_id'
assert 'standalone_movie_id' in cols, 'Missing standalone_movie_id'
print('All standalone columns present')
# Test CRUD
from db.standalone import upsert_watched_folder, get_watched_folders, upsert_standalone_series, get_standalone_series, cache_metadata, get_cached_metadata
fid = upsert_watched_folder('/test/media', label='Test')
folders = get_watched_folders()
assert len(folders) >= 1
sid = upsert_standalone_series('Test Series', '/test/media/Show', year=2024)
series = get_standalone_series(sid)
assert series['title'] == 'Test Series'
cache_metadata('test:key', 'tmdb', '{\"id\": 1}', ttl_days=1)
cached = get_cached_metadata('test:key')
assert cached is not None
print('All CRUD operations work')
"
  ```
  </verify>
  <done>4 new tables created in schema, wanted_items has standalone_series_id and standalone_movie_id columns, all CRUD operations in db/standalone.py work correctly with thread-safe locking.</done>
</task>

<task type="auto">
  <name>Task 2: Config settings and media file parser</name>
  <files>backend/config.py, backend/standalone/__init__.py, backend/standalone/parser.py</files>
  <action>
  1. In `backend/config.py` Settings class, add standalone config fields (after the existing fields, before the `model_config` class):

  ```python
  # Standalone Mode
  standalone_enabled: bool = False
  standalone_scan_interval_hours: int = 6  # 0 = disabled
  standalone_debounce_seconds: int = 10
  tmdb_api_key: str = ""  # TMDB API v3 Bearer token
  tvdb_api_key: str = ""  # TVDB API v4 key (optional)
  tvdb_pin: str = ""  # TVDB PIN (optional)
  metadata_cache_ttl_days: int = 30
  ```

  2. Create `backend/standalone/__init__.py` as a minimal package init (the StandaloneManager will be added in Plan 03):
  ```python
  """Standalone mode package -- folder-watch operation without Sonarr/Radarr.

  Provides filesystem watching (watcher.py), media file parsing (parser.py),
  and directory scanning (scanner.py) for standalone subtitle management.
  """
  ```

  3. Create `backend/standalone/parser.py` implementing the MediaFileParser:

  ```python
  """Media file parser -- extracts metadata from video filenames using guessit.

  Handles both standard naming (S01E02) and anime naming (absolute episodes,
  [Group] prefixes). Uses the full file path for series name context from
  parent directories.
  """
  ```

  Implement these functions:

  - `VIDEO_EXTENSIONS` constant: set of `.mkv`, `.mp4`, `.avi`, `.m4v`, `.wmv`, `.flv`, `.webm`, `.ts` (lowercase)

  - `ANIME_RELEASE_GROUPS` constant: set of known anime fansub groups for detection (e.g., "SubsPlease", "Erai-raws", "HorribleSubs", "Judas", "EMBER", "ASW", "Tsundere-Raws", "DameDesuYo", "GJM", "Commie", "Underwater", "Coalgirls", "Kametsu")

  - `is_video_file(path: str) -> bool` -- check extension against VIDEO_EXTENSIONS

  - `detect_anime_indicators(filename: str) -> bool` -- returns True if filename has anime indicators:
    * Square bracket group prefix: `[GroupName]`
    * Known anime release group in ANIME_RELEASE_GROUPS
    * Absolute episode numbering without season (e.g., "Title - 153.mkv")
    * CRC32 hash in brackets: `[A1B2C3D4]`

  - `parse_media_file(file_path: str) -> dict` -- the main parser:
    * Extract filename from path
    * Get parent directory name(s) for series context
    * Check anime indicators first
    * Call guessit with `{'type': 'episode', 'episode_prefer_number': True}` if anime indicators detected
    * Otherwise call guessit with `{'type': 'episode'}` first, then fall back to `{'type': 'movie'}` if no episode info found
    * Use parent directory name as series title fallback when guessit can't extract it from filename
    * Return dict with keys: `type` ("episode" or "movie"), `title`, `season`, `episode`, `absolute_episode`, `year`, `release_group`, `source`, `resolution`, `video_codec`, `is_anime` (bool), `confidence` ("high", "medium", "low" based on how much was parsed)

  - `group_files_by_series(file_paths: list[str]) -> dict[str, list[dict]]` -- groups parsed files by normalized series title (lowercase, stripped). Returns {normalized_title: [parsed_file_info_with_path]}

  Use `logging.getLogger(__name__)` for all logging. Docstrings on all public functions.
  </action>
  <verify>
  ```bash
  cd backend && python -c "
from standalone.parser import parse_media_file, detect_anime_indicators, is_video_file, group_files_by_series

# Test standard naming
r1 = parse_media_file('/media/The.Mandalorian.S03E05.720p.mkv')
assert r1['type'] == 'episode', f'Expected episode, got {r1[\"type\"]}'
assert r1['season'] == 3 or r1['episode'] == 5, f'Season/ep wrong: {r1}'
print(f'Standard: {r1[\"title\"]} S{r1[\"season\"]}E{r1[\"episode\"]}')

# Test anime naming
r2 = parse_media_file('/media/[SubsPlease] Jujutsu Kaisen - 42 (1080p).mkv')
assert r2['is_anime'] == True, 'Should detect anime'
print(f'Anime: {r2[\"title\"]} ep{r2.get(\"episode\") or r2.get(\"absolute_episode\")}')

# Test anime indicators
assert detect_anime_indicators('[SubsPlease] Title - 01.mkv') == True
assert detect_anime_indicators('Title.S01E01.mkv') == False

# Test video file detection
assert is_video_file('test.mkv') == True
assert is_video_file('test.srt') == False

# Test movie
r3 = parse_media_file('/media/The.Matrix.1999.1080p.BluRay.mkv')
assert r3['type'] == 'movie' or r3['year'] == 1999
print(f'Movie: {r3[\"title\"]} ({r3.get(\"year\")})')

# Test grouping
files = ['/media/Show/Show.S01E01.mkv', '/media/Show/Show.S01E02.mkv']
groups = group_files_by_series(files)
assert len(groups) >= 1, f'Expected 1 group, got {len(groups)}'
print(f'Grouped {len(files)} files into {len(groups)} series')
print('All parser tests pass')
"
  ```
  </verify>
  <done>Config has standalone_enabled, tmdb_api_key, tvdb_api_key, tvdb_pin, metadata_cache_ttl_days fields. Parser correctly parses standard TV (S01E02), anime ([Group] Title - 42), and movie filenames. Anime detection identifies fansub groups and absolute numbering.</done>
</task>

</tasks>

<verification>
- `python -c "from db.standalone import upsert_watched_folder; print('DB module imports')"` succeeds
- `python -c "from standalone.parser import parse_media_file; print('Parser imports')"` succeeds
- `python -c "from config import get_settings; s = get_settings(); print(s.standalone_enabled)"` prints False
- All 4 new tables queryable: `SELECT 1 FROM watched_folders LIMIT 1` etc.
- wanted_items has standalone_series_id and standalone_movie_id columns
</verification>

<success_criteria>
1. Four new database tables (watched_folders, standalone_series, standalone_movies, metadata_cache) exist with correct schema
2. wanted_items table has standalone_series_id and standalone_movie_id columns via migration
3. db/standalone.py provides full CRUD operations for all 4 tables with thread-safe locking
4. config.py has standalone_enabled, tmdb_api_key, tvdb_api_key, tvdb_pin, metadata_cache_ttl_days settings
5. standalone/parser.py correctly parses standard TV, anime, and movie filenames using guessit
6. Anime detection identifies [Group] prefixes, known fansub groups, and absolute episode numbering
</success_criteria>

<output>
After completion, create `.planning/phases/05-standalone-mode/05-01-SUMMARY.md`
</output>
