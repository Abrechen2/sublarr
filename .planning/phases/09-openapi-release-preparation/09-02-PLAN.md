---
phase: 09-openapi-release-preparation
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/wanted_scanner.py
  - backend/wanted_search.py
  - backend/routes/system.py
autonomous: true

must_haves:
  truths:
    - "Wanted scan supports incremental mode that only rescans items changed since last scan, with periodic full scan fallback"
    - "Wanted search processes multiple items in parallel using ThreadPoolExecutor instead of sequential time.sleep"
    - "GET /health/detailed returns subsystem status for translation backends, media servers, whisper backends, Sonarr/Radarr, and scheduler"
  artifacts:
    - path: "backend/wanted_scanner.py"
      provides: "Incremental scan with timestamp tracking and parallel ffprobe"
      contains: "last_scan_at"
    - path: "backend/wanted_search.py"
      provides: "Parallel item processing with bounded ThreadPoolExecutor"
      contains: "ThreadPoolExecutor"
    - path: "backend/routes/system.py"
      provides: "Extended /health/detailed with 5+ subsystem categories"
      contains: "translation_backends"
  key_links:
    - from: "backend/wanted_scanner.py"
      to: "backend/db/wanted.py"
      via: "timestamp tracking for incremental scan"
      pattern: "last_scan_at"
    - from: "backend/wanted_search.py"
      to: "backend/providers/__init__.py"
      via: "parallel search via ThreadPoolExecutor"
      pattern: "ThreadPoolExecutor"
---

<objective>
Optimize backend performance: make wanted scan incremental with timestamp tracking, parallelize wanted search item processing, and extend /health/detailed to cover all subsystems (translation backends, media servers, whisper, Sonarr/Radarr connectivity, scheduler status).

Purpose: Large libraries with thousands of episodes suffer from full-rescan overhead. Sequential item processing with 0.5s sleep is the bottleneck for wanted search. The health endpoint needs to report on all subsystems for monitoring and Swagger documentation.

Output: Incremental wanted scan, parallel wanted search, comprehensive /health/detailed endpoint.
</objective>

<execution_context>
@C:/Users/Dennis Wittke/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Dennis Wittke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-openapi-release-preparation/09-RESEARCH.md
@backend/wanted_scanner.py
@backend/wanted_search.py
@backend/routes/system.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Incremental wanted scan + parallel wanted search</name>
  <files>
    backend/wanted_scanner.py
    backend/wanted_search.py
  </files>
  <action>
**Incremental Wanted Scan (wanted_scanner.py):**

1. Add instance-level state tracking:
   - `self._last_scan_at` (datetime or None) -- timestamp of last successful scan
   - `self._scan_count` (int) -- counter for forcing full scan every N cycles
   - `FULL_SCAN_INTERVAL = 6` -- every 6th scan is a full scan regardless

2. Modify `scan_all()` (or equivalent main scan method):
   - Accept `incremental=True` parameter (default True)
   - If incremental and _last_scan_at is set and _scan_count % FULL_SCAN_INTERVAL != 0:
     - For Sonarr: filter series by comparing series `updated` timestamp against _last_scan_at
     - For Radarr: filter movies by comparing `movieFile.dateAdded` or `lastSearchTime` against _last_scan_at
     - Only process items modified since _last_scan_at
   - Else: run full scan (all series/movies)
   - After successful scan: update _last_scan_at to current time, increment _scan_count
   - Log whether scan was incremental or full, and item count

3. Parallel ffprobe:
   - Where ffprobe is called for subtitle detection (checking embedded subs), use a bounded ThreadPoolExecutor (max_workers=4)
   - Wrap individual ffprobe calls in executor.submit(), collect results with as_completed()
   - This avoids sequential subprocess spawning which is the main bottleneck
   - If ffprobe calls are inside a loop processing episodes, batch them per series

4. Add `force_full_scan()` method that resets _last_scan_at to None and calls scan_all(incremental=False)

**Parallel Wanted Search (wanted_search.py):**

1. In the main search function (search_all or equivalent):
   - Replace the sequential loop with `time.sleep(0.5)` between items with a bounded ThreadPoolExecutor
   - `max_workers = min(4, len(eligible_items))` to avoid over-parallelization
   - Use `concurrent.futures.as_completed()` to process results as they finish
   - Emit progress updates via WebSocket after each item completes (not just at end)

2. Keep rate limiting at the provider level (ProviderManager already handles this), but remove the artificial 0.5s sleep between items. The providers' own rate limiters and circuit breakers are sufficient.

3. Add error isolation: wrap each item's processing in try/except so one failure doesn't abort the batch. Log the error and continue with remaining items.

4. Keep the batch cancel mechanism working -- check `_cancel_flag` or equivalent between item completions.

**Important:** Do NOT change the public API (function signatures callable from routes). The parallelization is internal. The scan_all and search_all interfaces stay the same for callers.
  </action>
  <verify>
Run: `cd backend && python -c "
from wanted_scanner import WantedScanner
from wanted_search import WantedSearcher
import inspect

# Verify incremental support
scanner_src = inspect.getsource(WantedScanner)
assert 'incremental' in scanner_src or 'last_scan_at' in scanner_src, 'Missing incremental scan support'
print('OK: Incremental scan support found')

# Verify parallel search
searcher_src = inspect.getsource(WantedSearcher) if 'WantedSearcher' in dir() else open('wanted_search.py').read()
assert 'ThreadPoolExecutor' in searcher_src or 'executor' in searcher_src, 'Missing parallel search'
print('OK: Parallel search support found')
print('All checks passed')
"`
  </verify>
  <done>
  - Wanted scan supports incremental mode with _last_scan_at timestamp tracking
  - Full scan forced every 6th cycle as safety fallback
  - force_full_scan() method available for manual triggers
  - ffprobe calls batched with ThreadPoolExecutor (max 4 workers)
  - Wanted search uses ThreadPoolExecutor instead of sequential + sleep
  - Error isolation per item (one failure doesn't abort batch)
  - No public API changes
  </done>
</task>

<task type="auto">
  <name>Task 2: Extend /health/detailed with all subsystem checks</name>
  <files>
    backend/routes/system.py
  </files>
  <action>
Extend the existing `/health/detailed` endpoint in `backend/routes/system.py` to add 5 new subsystem categories. The endpoint already checks database, ollama, providers, disk, and memory.

**Add these subsystem checks (each wrapped in try/except returning {healthy: False, message: str(exc)} on error):**

1. **translation_backends**: Import TranslationManager (from translation module), call list_backends(), for each backend call health_check() if available, aggregate into:
   ```python
   subsystems["translation_backends"] = {
       "healthy": any(b["healthy"] for b in backends.values()) if backends else True,
       "backends": backends,  # {name: {"healthy": bool, "message": str}}
   }
   ```

2. **media_servers**: Import MediaServerManager (from mediaserver module), call health_check_all() or iterate instances, aggregate into:
   ```python
   subsystems["media_servers"] = {
       "healthy": all(c["healthy"] for c in checks) if checks else True,
       "instances": checks,  # [{type, name, healthy, message}]
   }
   ```

3. **whisper_backends**: Import WhisperManager (from whisper module), check active backend health, aggregate:
   ```python
   subsystems["whisper_backends"] = {
       "healthy": ...,
       "active_backend": ...,
       "message": ...,
   }
   ```

4. **arr_connectivity**: Check Sonarr and Radarr instance connectivity. Import sonarr_client/radarr_client or use config to build instance URLs, try a simple GET /system/status for each configured instance:
   ```python
   subsystems["arr_connectivity"] = {
       "healthy": all instances reachable,
       "sonarr": [{instance_name, healthy, message}],
       "radarr": [{instance_name, healthy, message}],
   }
   ```

5. **scheduler**: Report status of background schedulers (wanted scan timer, wanted search timer, backup scheduler if configured):
   ```python
   subsystems["scheduler"] = {
       "healthy": True,  # scheduler itself is always "healthy" if running
       "tasks": [
           {"name": "wanted_scan", "running": bool, "last_run": iso_str_or_null, "interval_hours": float},
           {"name": "wanted_search", "running": bool, "last_run": iso_str_or_null, "interval_hours": float},
           {"name": "backup", "enabled": bool, "last_run": iso_str_or_null},
       ],
   }
   ```

**Important:** All new checks must be wrapped in try/except. If a module is not configured or not available (e.g., no whisper backends configured), return healthy=True with an appropriate message like "No backends configured". Each subsystem check should have a short timeout -- don't let a single slow check block the entire response. Consider using a 5-second timeout for each external connectivity check (Sonarr/Radarr/media servers).

Also: Update the overall health status calculation to include the new subsystem checks in the "any unhealthy = overall unhealthy" logic.
  </action>
  <verify>
Run: `cd backend && python -c "
from app import create_app
app = create_app()
client = app.test_client()
resp = client.get('/api/v1/health/detailed')
data = resp.get_json()
subsystems = data.get('subsystems', {})
expected = ['translation_backends', 'media_servers', 'whisper_backends', 'arr_connectivity', 'scheduler']
found = [k for k in expected if k in subsystems]
print(f'Status: {resp.status_code}')
print(f'Subsystems found: {found}')
print(f'Missing: {[k for k in expected if k not in subsystems]}')
assert len(found) == 5, f'Expected 5 new subsystems, found {len(found)}'
print('All new subsystem checks present')
"`
  </verify>
  <done>
  - /health/detailed returns 5 new subsystem categories: translation_backends, media_servers, whisper_backends, arr_connectivity, scheduler
  - Each subsystem check is wrapped in try/except with graceful degradation
  - Unconfigured subsystems return healthy=True with explanatory message
  - Overall health status includes new subsystem checks
  - No regression on existing subsystem checks (database, ollama, providers, disk, memory)
  </done>
</task>

</tasks>

<verification>
1. Wanted scan with incremental=True processes fewer items than full scan on second run
2. Wanted search completes faster with parallel execution (no 0.5s sleep between items)
3. /health/detailed returns all subsystem categories (10+ total: existing 5 + new 5)
4. `cd backend && python -m pytest tests/ -x -q` passes
</verification>

<success_criteria>
- Incremental scan only processes changed items after initial full scan
- Parallel wanted search uses ThreadPoolExecutor with bounded concurrency
- /health/detailed covers all subsystems: DB, Ollama, providers, disk, memory, translation backends, media servers, whisper, arr connectivity, scheduler
</success_criteria>

<output>
After completion, create `.planning/phases/09-openapi-release-preparation/09-02-SUMMARY.md`
</output>
