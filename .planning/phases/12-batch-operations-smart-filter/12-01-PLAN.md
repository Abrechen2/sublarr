---
phase: 12-batch-operations-smart-filter
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/db/models/core.py
  - backend/db/repositories/wanted.py
  - backend/db/repositories/library.py
  - backend/db/repositories/search.py
  - backend/db/repositories/presets.py
  - backend/db/repositories/__init__.py
  - backend/db/search.py
  - backend/db/presets.py
  - backend/routes/search.py
  - backend/routes/filter_presets.py
  - backend/routes/__init__.py
  - backend/app.py
autonomous: true

must_haves:
  truths:
    - "FilterPreset ORM model exists with scope, conditions JSON, and is_default fields"
    - "FTS5 trigram virtual tables exist for series, episodes, and subtitles; populated at startup"
    - "GET /api/v1/search?q=&type= returns grouped results across series, episodes, subtitles"
    - "WantedRepository.get_wanted_items() accepts sort_by, sort_dir, search, and date range params"
    - "HistoryRepository.get_download_history() accepts format, language, score_min, score_max, search params"
    - "POST /api/v1/wanted/batch-action accepts item_ids + action for blacklist, ignore, and export"
    - "Full CRUD for filter presets: GET/POST/PUT/DELETE /api/v1/filter-presets"
  artifacts:
    - path: "backend/db/models/core.py"
      provides: "FilterPreset ORM model added"
      contains: "class FilterPreset"
    - path: "backend/db/repositories/search.py"
      provides: "SearchRepository with FTS5 trigram queries"
      contains: "class SearchRepository"
    - path: "backend/db/repositories/presets.py"
      provides: "FilterPresetsRepository CRUD"
      contains: "class FilterPresetsRepository"
    - path: "backend/routes/search.py"
      provides: "GET /api/v1/search endpoint"
      contains: "bp = Blueprint"
    - path: "backend/routes/filter_presets.py"
      provides: "CRUD endpoints for /api/v1/filter-presets"
      contains: "bp = Blueprint"
  key_links:
    - from: "backend/routes/search.py"
      to: "backend/db/search.py"
      via: "search_all() shim import"
      pattern: "from db.search import search_all"
    - from: "backend/routes/filter_presets.py"
      to: "backend/db/presets.py"
      via: "preset CRUD shim"
      pattern: "from db.presets import"
    - from: "backend/app.py"
      to: "backend/db/repositories/search.py"
      via: "init_search_tables() called after db.create_all()"
      pattern: "init_search_tables"
---

<objective>
Backend infrastructure for Phase 12: FilterPreset DB model, FTS5 global search, extended filter/sort params in WantedRepository and HistoryRepository, batch action endpoints, and filter preset CRUD API.

Purpose: Provide all backend foundations that Plans 02 and 03 consume through API calls. No frontend work in this plan.

Output: FilterPreset ORM model + Alembic migration, SearchRepository (FTS5 trigram), FilterPresetsRepository, extended repositories, 2 new route blueprints (search, filter_presets), batch-action endpoint extension in wanted.py.
</objective>

<execution_context>
@C:/Users/Dennis Wittke/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Dennis Wittke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/12-batch-operations-smart-filter/12-RESEARCH.md

@backend/db/models/core.py
@backend/db/repositories/base.py
@backend/db/repositories/wanted.py
@backend/db/repositories/library.py
@backend/db/repositories/__init__.py
@backend/routes/__init__.py
@backend/routes/wanted.py
@backend/routes/library.py
@backend/app.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: FilterPreset ORM model + Alembic migration</name>
  <files>
    backend/db/models/core.py
    backend/alembic/versions/
  </files>
  <action>
**Add FilterPreset model to `backend/db/models/core.py`:**

Add at the end of the file, following the exact patterns of existing models in that file:

```python
class FilterPreset(db.Model):
    """Saved filter configurations per page scope."""
    __tablename__ = "filter_presets"

    id:          Mapped[int] = mapped_column(Integer, primary_key=True, autoincrement=True)
    name:        Mapped[str] = mapped_column(String(100), nullable=False)
    scope:       Mapped[str] = mapped_column(String(50), nullable=False)   # 'wanted'|'library'|'history'
    conditions:  Mapped[str] = mapped_column(Text, nullable=False, default="{}")  # JSON condition tree
    is_default:  Mapped[int] = mapped_column(Integer, nullable=False, default=0)   # 1 = auto-apply
    created_at:  Mapped[str] = mapped_column(Text, nullable=False)
    updated_at:  Mapped[str] = mapped_column(Text, nullable=False)

    __table_args__ = (
        Index("idx_filter_presets_scope", "scope"),
    )
```

**Generate Alembic migration:**
Run `cd backend && alembic revision --autogenerate -m "add_filter_presets"` then verify the generated file adds the `filter_presets` table. If autogenerate is not available or fails, write the migration manually using the existing migration files as templates.
  </action>
  <verify>
`cd backend && python -c "from db.models.core import FilterPreset; print('FilterPreset OK')"` succeeds.
`cd backend && python -c "from extensions import db; from app import create_app; app = create_app(); app.app_context().push(); db.create_all(); print('tables OK')"` succeeds without error.
  </verify>
  <done>
FilterPreset model exists in core.py with id, name, scope, conditions (JSON text), is_default, created_at, updated_at. Index on scope. Migration file created. Model importable.
  </done>
</task>

<task type="auto">
  <name>Task 2: SearchRepository with FTS5 trigram tables</name>
  <files>
    backend/db/repositories/search.py
    backend/db/repositories/__init__.py
    backend/db/search.py
    backend/app.py
  </files>
  <action>
**Create `backend/db/repositories/search.py`:**

```python
"""Search repository — FTS5 trigram full-text search across all entities."""

import logging
from sqlalchemy import text
from db.repositories.base import BaseRepository

logger = logging.getLogger(__name__)

# FTS5 virtual table schema — created once at app startup
_SEARCH_SCHEMA = [
    """CREATE VIRTUAL TABLE IF NOT EXISTS search_series
       USING fts5(id UNINDEXED, title, tokenize="trigram")""",
    """CREATE VIRTUAL TABLE IF NOT EXISTS search_episodes
       USING fts5(id UNINDEXED, series_id UNINDEXED, title, season_episode, tokenize="trigram")""",
    """CREATE VIRTUAL TABLE IF NOT EXISTS search_subtitles
       USING fts5(id UNINDEXED, file_path, provider_name, language, tokenize="trigram")""",
]


class SearchRepository(BaseRepository):
    """Full-text search across series, episodes, and subtitles using FTS5."""

    def init_search_tables(self) -> None:
        """Create FTS5 virtual tables. Call from app.py after db.create_all()."""
        for stmt in _SEARCH_SCHEMA:
            self.session.execute(text(stmt))
        self._commit()

    def rebuild_index(self) -> None:
        """Rebuild FTS5 tables from subtitle_downloads. Call after library sync."""
        # subtitle_downloads is the primary DB-backed entity for search
        self.session.execute(text("DELETE FROM search_subtitles"))
        self.session.execute(text("""
            INSERT INTO search_subtitles(id, file_path, provider_name, language)
            SELECT id, file_path, provider_name, language
            FROM subtitle_downloads
        """))
        # wanted_items title search (series/episode titles come from here)
        self.session.execute(text("DELETE FROM search_episodes"))
        self.session.execute(text("""
            INSERT INTO search_episodes(id, series_id, title, season_episode)
            SELECT id, COALESCE(sonarr_series_id, radarr_movie_id, 0),
                   title, season_episode
            FROM wanted_items
            WHERE title IS NOT NULL AND title != ''
        """))
        # series search: deduplicated series titles from wanted_items
        self.session.execute(text("DELETE FROM search_series"))
        self.session.execute(text("""
            INSERT INTO search_series(id, title)
            SELECT sonarr_series_id, title
            FROM wanted_items
            WHERE sonarr_series_id IS NOT NULL AND title IS NOT NULL
            GROUP BY sonarr_series_id
        """))
        self._commit()

    def search_all(self, query: str, limit: int = 20) -> dict:
        """FTS5 trigram search across series, episodes, and subtitles.

        Returns grouped results: {"series": [...], "episodes": [...], "subtitles": [...]}.
        Minimum query length is 2 characters (trigram requires 3-char tokens for MATCH;
        LIKE '%q%' works for 2+ chars and uses the trigram index).
        """
        if not query or len(query.strip()) < 2:
            return {"series": [], "episodes": [], "subtitles": []}

        like_term = f"%{query.strip()}%"
        with self.session.bind.connect() as conn:
            series = conn.execute(
                text("SELECT id, title FROM search_series WHERE title LIKE :q LIMIT :lim"),
                {"q": like_term, "lim": limit}
            ).mappings().all()

            episodes = conn.execute(
                text("""SELECT id, series_id, title, season_episode
                        FROM search_episodes
                        WHERE title LIKE :q OR season_episode LIKE :q LIMIT :lim"""),
                {"q": like_term, "lim": limit}
            ).mappings().all()

            subtitles = conn.execute(
                text("""SELECT id, file_path, provider_name, language
                        FROM search_subtitles
                        WHERE file_path LIKE :q OR provider_name LIKE :q LIMIT :lim"""),
                {"q": like_term, "lim": limit}
            ).mappings().all()

        return {
            "series": [dict(r) for r in series],
            "episodes": [dict(r) for r in episodes],
            "subtitles": [dict(r) for r in subtitles],
        }
```

**Add to `backend/db/repositories/__init__.py`:**
Import `SearchRepository` and re-export it alongside other repositories.

**Create `backend/db/search.py` convenience shim:**
Follow the exact pattern of `backend/db/quality.py` (thin wrapper with `_repo` lazy init).
Export: `init_search_tables`, `rebuild_search_index`, `search_all`.

**Wire into `backend/app.py`:**
After the `db.create_all()` call in `create_app()`, add:
```python
from db.search import init_search_tables
init_search_tables()
```
  </action>
  <verify>
`cd backend && python -c "from db.search import init_search_tables, search_all; print('search shim OK')"` succeeds.
`cd backend && python -c "from db.repositories.search import SearchRepository; print('SearchRepository OK')"` succeeds.
  </verify>
  <done>
SearchRepository with init_search_tables(), rebuild_index(), and search_all() methods. FTS5 schema covers series (from wanted_items grouping), episodes (from wanted_items), and subtitles (from subtitle_downloads). Shim exports init_search_tables, rebuild_search_index, search_all. app.py calls init_search_tables() on startup.
  </done>
</task>

<task type="auto">
  <name>Task 3: FilterPresetsRepository + shim</name>
  <files>
    backend/db/repositories/presets.py
    backend/db/repositories/__init__.py
    backend/db/presets.py
  </files>
  <action>
**Create `backend/db/repositories/presets.py`:**

```python
"""Filter presets repository — CRUD for saved filter configurations."""

import json
import logging
from datetime import datetime, timezone
from sqlalchemy import select, delete

from db.models.core import FilterPreset
from db.repositories.base import BaseRepository

logger = logging.getLogger(__name__)

# Allowed filter field names per scope (prevents injection via preset conditions)
ALLOWED_FIELDS = {
    "wanted": {"status", "item_type", "subtitle_type", "target_language", "upgrade_candidate", "title"},
    "library": {"item_type", "language", "provider"},
    "history": {"provider_name", "language", "format", "score"},
}


class FilterPresetsRepository(BaseRepository):
    """CRUD for filter_presets table."""

    def list_presets(self, scope: str) -> list[dict]:
        stmt = select(FilterPreset).where(FilterPreset.scope == scope)
        rows = self.session.execute(stmt).scalars().all()
        return [self._preset_to_dict(r) for r in rows]

    def get_preset(self, preset_id: int) -> dict | None:
        row = self.session.get(FilterPreset, preset_id)
        return self._preset_to_dict(row) if row else None

    def create_preset(self, name: str, scope: str, conditions: dict, is_default: bool = False) -> dict:
        self._validate_conditions(conditions, scope)
        now = datetime.now(timezone.utc).isoformat()
        preset = FilterPreset(
            name=name,
            scope=scope,
            conditions=json.dumps(conditions),
            is_default=1 if is_default else 0,
            created_at=now,
            updated_at=now,
        )
        self.session.add(preset)
        self._commit()
        return self._preset_to_dict(preset)

    def update_preset(self, preset_id: int, name: str = None, conditions: dict = None,
                      is_default: bool = None) -> dict | None:
        row = self.session.get(FilterPreset, preset_id)
        if not row:
            return None
        if name is not None:
            row.name = name
        if conditions is not None:
            self._validate_conditions(conditions, row.scope)
            row.conditions = json.dumps(conditions)
        if is_default is not None:
            row.is_default = 1 if is_default else 0
        row.updated_at = datetime.now(timezone.utc).isoformat()
        self._commit()
        return self._preset_to_dict(row)

    def delete_preset(self, preset_id: int) -> bool:
        row = self.session.get(FilterPreset, preset_id)
        if not row:
            return False
        self.session.execute(delete(FilterPreset).where(FilterPreset.id == preset_id))
        self._commit()
        return True

    def _preset_to_dict(self, row: FilterPreset) -> dict:
        return {
            "id": row.id,
            "name": row.name,
            "scope": row.scope,
            "conditions": json.loads(row.conditions or "{}"),
            "is_default": bool(row.is_default),
            "created_at": row.created_at,
            "updated_at": row.updated_at,
        }

    def _validate_conditions(self, node: dict, scope: str) -> None:
        """Validate condition tree field names against allowlist (raises ValueError on violation)."""
        allowed = ALLOWED_FIELDS.get(scope, set())
        if "logic" in node:
            for child in node.get("conditions", []):
                self._validate_conditions(child, scope)
        else:
            field = node.get("field", "")
            if field not in allowed:
                raise ValueError(f"Field '{field}' not allowed for scope '{scope}'")
```

**Add to `backend/db/repositories/__init__.py`:**
Import `FilterPresetsRepository` alongside other repositories.

**Also add `build_clause()` helper to `FilterPresetsRepository`:**

This function converts a stored `conditions` JSON tree into a SQLAlchemy WHERE clause. It is needed to apply presets at query time (not just display them client-side).

```python
from sqlalchemy import and_, or_

SUPPORTED_OPERATORS = {
    "eq":       lambda col, val: col == val,
    "neq":      lambda col, val: col != val,
    "contains": lambda col, val: col.ilike(f"%{val}%"),
    "starts":   lambda col, val: col.ilike(f"{val}%"),
    "gt":       lambda col, val: col > val,
    "lt":       lambda col, val: col < val,
    "in":       lambda col, val: col.in_(val if isinstance(val, list) else [val]),
}

def build_clause(self, node: dict, field_map: dict):
    """Recursively build a SQLAlchemy clause from a condition tree node.

    Args:
        node: Either {"field", "op", "value"} leaf or {"logic": "AND"|"OR", "conditions": [...]} group
        field_map: Maps field name strings to SQLAlchemy column objects

    Returns:
        SQLAlchemy BinaryExpression or BooleanClauseList

    Raises:
        ValueError: If field or operator not in allowed maps
    """
    if "logic" in node:
        sub_clauses = [self.build_clause(c, field_map) for c in node.get("conditions", [])]
        if not sub_clauses:
            return and_()  # empty group = no restriction
        combinator = and_ if node["logic"].upper() == "AND" else or_
        return combinator(*sub_clauses)
    else:
        field_name = node.get("field", "")
        op_name = node.get("op", "")
        value = node.get("value")
        if field_name not in field_map:
            raise ValueError(f"Unknown filter field: {field_name}")
        if op_name not in SUPPORTED_OPERATORS:
            raise ValueError(f"Unknown filter operator: {op_name}")
        return SUPPORTED_OPERATORS[op_name](field_map[field_name], value)
```

**Create `backend/db/presets.py` shim:**
Follow `db/quality.py` pattern. Exports: `list_presets`, `get_preset`, `create_preset`, `update_preset`, `delete_preset`, `build_preset_clause`.

The `build_preset_clause(preset_id, field_map)` convenience function loads a preset by ID and returns its build_clause() result, ready to be passed to `.where()`.
  </action>
  <verify>
`cd backend && python -c "from db.repositories.presets import FilterPresetsRepository; print('FilterPresetsRepository OK')"` succeeds.
`cd backend && python -c "from db.presets import list_presets, create_preset, delete_preset, build_preset_clause; print('presets shim OK')"` succeeds.
  </verify>
  <done>
FilterPresetsRepository with list, get, create, update, delete, build_clause methods. Field allowlist + operator allowlist prevent injection. build_clause() supports nested AND/OR trees. Shim exports 6 functions including build_preset_clause. Both importable without errors.
  </done>
</task>

<task type="auto">
  <name>Task 4: Extend WantedRepository with sort + text search + date range</name>
  <files>
    backend/db/repositories/wanted.py
  </files>
  <action>
Extend `WantedRepository.get_wanted_items()` with three new optional parameters:
- `sort_by: str = "added_at"` — field to sort by (allowed: `added_at`, `title`, `last_search_at`, `current_score`, `search_count`)
- `sort_dir: str = "desc"` — direction (allowed: `asc`, `desc`)
- `search: str | None = None` — text search against `title` and `file_path` (ilike `%search%`)

**Sort field allowlist:**
```python
SORT_FIELDS = {
    "added_at":      WantedItem.added_at,
    "title":         WantedItem.title,
    "last_search_at": WantedItem.last_search_at,
    "current_score": WantedItem.current_score,
    "search_count":  WantedItem.search_count,
}
```

If `sort_by` is not in `SORT_FIELDS`, silently fall back to `added_at`.
If `sort_dir` is not `asc`, treat as `desc`.

**Text search condition:**
```python
if search:
    search_term = f"%{search}%"
    conditions.append(
        or_(
            WantedItem.title.ilike(search_term),
            WantedItem.file_path.ilike(search_term),
        )
    )
```

Apply sorting to `data_stmt` using `.order_by(asc(col))` or `.order_by(desc(col))` from sqlalchemy imports.

Also add `asc` and `desc` to the sqlalchemy imports at the top of the file.

**Also add `preset_conditions: dict | None = None` parameter.** When provided (non-None), call `FilterPresetsRepository().build_clause(preset_conditions, WANTED_FIELDS)` and apply the resulting clause to both the count and data statements via `.where(clause)`. This enables preset-driven AND/OR filtering at query time. Import `FilterPresetsRepository` lazily inside the function body (not at module level) to avoid circular imports.

Do NOT change the function signature in a way that breaks existing callers — all new params are keyword-only with defaults.
  </action>
  <verify>
`cd backend && python -c "from db.repositories.wanted import WantedRepository; print('wanted repo OK')"` succeeds.
Existing tests in `backend/tests/` still pass: `cd backend && python -m pytest tests/ -x -q 2>&1 | head -20`.
  </verify>
  <done>
get_wanted_items() accepts sort_by, sort_dir, search params. Sort uses allowlist. Text search uses ilike on title + file_path with OR. Existing callers unaffected (all new params have defaults).
  </done>
</task>

<task type="auto">
  <name>Task 5: Extend HistoryRepository (download_history) with format, score, and search filters</name>
  <files>
    backend/db/repositories/library.py
  </files>
  <action>
Locate `get_download_history()` (or equivalent) in `backend/db/repositories/library.py`.

Add these optional parameters with defaults:
- `format: str | None = None` — filter by subtitle format (`ass`, `srt`, etc.)
- `score_min: int | None = None` — filter where score >= score_min
- `score_max: int | None = None` — filter where score <= score_max
- `search: str | None = None` — text search against `file_path` and `provider_name` (ilike)
- `sort_by: str = "downloaded_at"` — allowed: `downloaded_at`, `score`, `provider_name`, `language`
- `sort_dir: str = "desc"`

Apply the same allowlist + ilike pattern from Task 4.

If the `SubtitleDownload` model does not have a `score` column, check `db/models/core.py` first — if absent, skip the `score_min`/`score_max` filters but still add the other four.
  </action>
  <verify>
`cd backend && python -c "from db.repositories.library import LibraryRepository; print('library repo OK')"` succeeds.
  </verify>
  <done>
get_download_history() accepts format, score_min, score_max, search, sort_by, sort_dir. All new params have defaults. Allowlist enforced for sort. ilike used for text search.
  </done>
</task>

<task type="auto">
  <name>Task 6: POST /wanted/batch-action — bulk blacklist, ignore, export</name>
  <files>
    backend/routes/wanted.py
  </files>
  <action>
Add a new endpoint `POST /api/v1/wanted/batch-action` to `backend/routes/wanted.py`.

**Request body:**
```json
{
  "item_ids": [1, 2, 3],
  "action": "blacklist" | "ignore" | "unignore" | "export"
}
```

**Action implementations:**

`ignore`: Set `status = "ignored"` for all matching IDs.
`unignore`: Set `status = "wanted"` for all IDs currently with status `"ignored"`.
`blacklist`: For each item_id, call `blacklist_item()` if it exists in `db/blacklist.py` or equivalent; otherwise set status `"ignored"` with a note. Do not hard-fail if blacklist module doesn't exist — fall back to ignore with a `warning` field in the response.
`export`: Return a JSON array of the full wanted item dicts for the given IDs. No DB changes.

**Response (non-export):**
```json
{
  "success": true,
  "action": "ignore",
  "affected": 3,
  "item_ids": [1, 2, 3]
}
```

**Validation:**
- `item_ids` must be a non-empty list of integers (max 500)
- `action` must be one of the four allowed values
- Return 400 with `{"error": "..."}` for invalid input

Use `from db.repositories.wanted import WantedRepository` and follow the existing request-scoped repository pattern used in other routes in this file.
  </action>
  <verify>
Flask app starts without import error: `cd backend && python -c "from app import create_app; app = create_app(); print('app OK')"`.
The new endpoint appears in `flask routes` output or can be accessed at the correct URL.
  </verify>
  <done>
POST /api/v1/wanted/batch-action exists and handles ignore, unignore, blacklist, export actions. Validates item_ids list (max 500). Returns affected count. No crash if blacklist module absent.
  </done>
</task>

<task type="auto">
  <name>Task 7: Global search route + filter presets route</name>
  <files>
    backend/routes/search.py
    backend/routes/filter_presets.py
    backend/routes/__init__.py
  </files>
  <action>
**Create `backend/routes/search.py`:**

```python
"""Global search route — GET /api/v1/search"""

from flask import Blueprint, request, jsonify
from db.search import search_all, rebuild_search_index

bp = Blueprint("search", __name__, url_prefix="/api/v1")


@bp.route("/search", methods=["GET"])
def global_search():
    """Full-text search across series, episodes, and subtitles.
    ---
    get:
      tags: [Search]
      summary: Global search
      parameters:
        - in: query
          name: q
          schema: {type: string}
          description: Search query (min 2 chars)
        - in: query
          name: limit
          schema: {type: integer, default: 20}
      responses:
        200:
          description: Grouped search results
    """
    q = request.args.get("q", "").strip()
    limit = min(int(request.args.get("limit", 20)), 50)
    if not q or len(q) < 2:
        return jsonify({"series": [], "episodes": [], "subtitles": [], "query": q})
    results = search_all(q, limit=limit)
    results["query"] = q
    return jsonify(results)


@bp.route("/search/rebuild-index", methods=["POST"])
def rebuild_index():
    """Rebuild FTS5 search index from current DB state."""
    rebuild_search_index()
    return jsonify({"success": True, "message": "Search index rebuilt"})
```

**Create `backend/routes/filter_presets.py`:**

```python
"""Filter presets routes — /api/v1/filter-presets"""

import json
from flask import Blueprint, request, jsonify
from db.presets import list_presets, get_preset, create_preset, update_preset, delete_preset

bp = Blueprint("filter_presets", __name__, url_prefix="/api/v1")


@bp.route("/filter-presets", methods=["GET"])
def list_filter_presets():
    scope = request.args.get("scope", "wanted")
    return jsonify(list_presets(scope))


@bp.route("/filter-presets", methods=["POST"])
def create_filter_preset():
    data = request.get_json() or {}
    name = data.get("name", "").strip()
    scope = data.get("scope", "")
    conditions = data.get("conditions", {})
    is_default = bool(data.get("is_default", False))
    if not name or scope not in ("wanted", "library", "history"):
        return jsonify({"error": "name and valid scope required"}), 400
    try:
        preset = create_preset(name, scope, conditions, is_default)
        return jsonify(preset), 201
    except ValueError as e:
        return jsonify({"error": str(e)}), 422


@bp.route("/filter-presets/<int:preset_id>", methods=["PUT"])
def update_filter_preset(preset_id: int):
    data = request.get_json() or {}
    try:
        preset = update_preset(preset_id, **{k: v for k, v in data.items()
                                              if k in ("name", "conditions", "is_default")})
    except ValueError as e:
        return jsonify({"error": str(e)}), 422
    if not preset:
        return jsonify({"error": "Not found"}), 404
    return jsonify(preset)


@bp.route("/filter-presets/<int:preset_id>", methods=["DELETE"])
def delete_filter_preset(preset_id: int):
    if not delete_preset(preset_id):
        return jsonify({"error": "Not found"}), 404
    return jsonify({"success": True})
```

**Register both blueprints in `backend/routes/__init__.py`:**
Import `search.bp` and `filter_presets.bp` and add them to the `register_blueprints(app)` function alongside existing blueprints.
  </action>
  <verify>
`cd backend && python -c "from app import create_app; app = create_app(); print([r.rule for r in app.url_map.iter_rules() if 'search' in r.rule or 'filter-presets' in r.rule])"` shows the expected routes.
  </verify>
  <done>
GET /api/v1/search, POST /api/v1/search/rebuild-index registered. GET/POST /api/v1/filter-presets, PUT/DELETE /api/v1/filter-presets/<id> registered. Both blueprints imported in routes/__init__.py.
  </done>
</task>

<task type="auto">
  <name>Task 8: Wire sort/search params into routes + update shims</name>
  <files>
    backend/routes/wanted.py
    backend/routes/library.py
    backend/db/wanted.py
    backend/db/library.py
  </files>
  <action>
**Step A — Read the shim files first** to understand their current signatures. The shims (`db/wanted.py`, `db/library.py`) are thin wrappers that call the repository. Routes import from the shim, not the repository directly.

**Step B — Update `backend/db/wanted.py` shim:**
Find the `get_wanted_items(...)` function in this shim and add the new keyword parameters with defaults so the signature becomes:
```python
def get_wanted_items(page=1, per_page=50, item_type=None, status=None,
                     series_id=None, subtitle_type=None,
                     sort_by="added_at", sort_dir="desc", search=None):
```
Pass all parameters through to `_get_repo().get_wanted_items(...)`.

**Step C — Update `backend/db/library.py` shim:**
Find the `get_download_history(...)` function and add new keyword params with defaults:
```python
def get_download_history(page=1, per_page=50, provider=None, language=None,
                         format=None, score_min=None, score_max=None,
                         search=None, sort_by="downloaded_at", sort_dir="desc"):
```
Pass all parameters through to `_get_repo().get_download_history(...)`.

**Step D — Update `backend/routes/wanted.py`:**
In the `GET /api/v1/wanted` handler, extract:
```python
sort_by = request.args.get("sort_by", "added_at")
sort_dir = request.args.get("sort_dir", "desc")
search = request.args.get("search") or None
```
Pass these to `get_wanted_items(...)` alongside the existing params.

**Step E — Update `backend/routes/library.py`:**
In the `GET /api/v1/library` (or history) handler, extract:
```python
format_filter = request.args.get("format") or None
score_min = request.args.get("score_min", type=int)
score_max = request.args.get("score_max", type=int)
search = request.args.get("search") or None
sort_by = request.args.get("sort_by", "downloaded_at")
sort_dir = request.args.get("sort_dir", "desc")
```
Pass to `get_download_history(...)`.

Update the OpenAPI YAML docstrings in both route files to document the new query parameters.
  </action>
  <verify>
`cd backend && python -m pytest tests/ -x -q 2>&1 | head -30` — all existing tests pass.
Flask app starts: `cd backend && python -c "from app import create_app; app = create_app(); print('app OK')"`.
`cd backend && python -c "from db.wanted import get_wanted_items; import inspect; sig = inspect.signature(get_wanted_items); assert 'sort_by' in sig.parameters; print('wanted shim OK')"` succeeds.
`cd backend && python -c "from db.library import get_download_history; import inspect; sig = inspect.signature(get_download_history); assert 'sort_by' in sig.parameters; print('library shim OK')"` succeeds.
  </verify>
  <done>
db/wanted.py shim accepts sort_by, sort_dir, search and forwards them to WantedRepository. db/library.py shim accepts format, score_min, score_max, search, sort_by, sort_dir and forwards them to LibraryRepository. Routes extract and pass the new params. All existing tests pass.
  </done>
</task>

</tasks>
