---
phase: 08-i18n-backup-admin-polish
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/routes/system.py
  - backend/routes/tools.py
autonomous: true

must_haves:
  truths:
    - "POST /api/v1/backup/full creates a ZIP containing manifest.json, config.json, and sublarr.db"
    - "GET /api/v1/backup/full/download/<filename> returns the ZIP file for browser download"
    - "POST /api/v1/backup/restore accepts a ZIP upload and restores config + database"
    - "GET /api/v1/statistics returns daily stats, provider stats, download stats with time range filter"
    - "GET /api/v1/statistics/export returns CSV or JSON export of statistics data"
    - "GET /api/v1/logs/download returns the log file as a downloadable attachment"
    - "PUT /api/v1/logs/rotation updates log rotation config (max_size, backup_count)"
    - "POST /api/v1/tools/remove-hi removes HI markers from a subtitle file"
    - "POST /api/v1/tools/adjust-timing shifts subtitle timestamps by offset_ms"
    - "POST /api/v1/tools/common-fixes applies encoding and whitespace fixes"
  artifacts:
    - path: "backend/routes/system.py"
      provides: "ZIP backup/restore endpoints, statistics endpoint, log download/rotation"
      contains: "backup/full"
    - path: "backend/routes/tools.py"
      provides: "Subtitle processing tool API endpoints"
      exports: ["bp"]
  key_links:
    - from: "backend/routes/system.py"
      to: "backend/database_backup.py"
      via: "DatabaseBackup.create_backup() for ZIP contents"
      pattern: "DatabaseBackup"
    - from: "backend/routes/system.py"
      to: "backend/db/jobs.py"
      via: "get_stats_summary() for statistics"
      pattern: "get_stats_summary"
    - from: "backend/routes/system.py"
      to: "backend/routes/config.py"
      via: "get_safe_config() for ZIP config export"
      pattern: "get_safe_config"
    - from: "backend/routes/tools.py"
      to: "backend/hi_remover.py"
      via: "remove_hi_markers() for HI removal tool"
      pattern: "remove_hi_markers"
    - from: "backend/routes/tools.py"
      to: "backend/ass_utils.py"
      via: "fix_line_breaks() for common fixes tool"
      pattern: "fix_line_breaks"
---

<objective>
Add backend API endpoints for ZIP backup/restore, comprehensive statistics with time-range filtering, log download/rotation config, and subtitle processing tools.

Purpose: These endpoints provide the data and functionality that the frontend plans (03, 04, 05) will consume. By building all backend APIs in one plan, the frontend plans can work in parallel without blocked dependencies.
Output: 10+ new API endpoints across system.py and a new tools.py blueprint.
</objective>

<execution_context>
@C:/Users/Dennis Wittke/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Dennis Wittke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-i18n-backup-admin-polish/08-RESEARCH.md
@backend/routes/system.py
@backend/routes/config.py
@backend/database_backup.py
@backend/db/jobs.py
@backend/db/providers.py
@backend/hi_remover.py
@backend/ass_utils.py
@backend/app.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: ZIP backup/restore endpoints, statistics API, log download/rotation</name>
  <files>backend/routes/system.py</files>
  <action>
Add the following endpoints to `backend/routes/system.py`. Import `io`, `json`, `zipfile`, `csv` at the top. All new endpoints go AFTER the existing `restore_backup` endpoint.

**ZIP Backup -- POST /api/v1/backup/full:**
1. Create DB backup using existing `DatabaseBackup.create_backup(label="manual")`
2. Build config export using `get_settings().get_safe_config()`
3. Create ZIP in memory using `io.BytesIO()` + `zipfile.ZipFile(buffer, 'w', zipfile.ZIP_DEFLATED)`
4. Add `manifest.json` with: `version` (from config), `created_at` (ISO timestamp), `schema_version: 1`, `contents` list
5. Add `config.json` with the safe config export (json.dumps with indent=2)
6. Add `sublarr.db` by reading the backup file path from step 1
7. Save ZIP to the backup_dir with name `sublarr_full_<timestamp>.zip`
8. Return 201 with `{ filename, size_bytes, created_at, contents: ['manifest.json', 'config.json', 'sublarr.db'] }`
9. IMPORTANT: `buffer.seek(0)` before writing to disk

**ZIP Download -- GET /api/v1/backup/full/download/\<filename\>:**
1. Validate filename ends with `.zip` and contains no path separators (security)
2. Build full path from `backup_dir / filename`
3. Check file exists, return 404 if not
4. Return `send_file(path, mimetype='application/zip', as_attachment=True, download_name=filename)`

**ZIP Restore -- POST /api/v1/backup/full/restore:**
1. Accept file upload via `request.files['file']` (multipart/form-data)
2. Validate it's a valid ZIP: `zipfile.is_zipfile(file_stream)`
3. Open ZIP, read `manifest.json`, validate `schema_version == 1`
4. Extract and import config: read `config.json`, call the same import logic as `routes/config.py import_config()` -- import non-secret keys via `save_config_entry()`, skip secrets
5. If ZIP contains `sublarr.db`: save to a temp file, then call `DatabaseBackup.restore_backup(temp_path)` (which handles safety backup internally)
6. Reload settings and invalidate caches (same pattern as `import_config()`)
7. Return `{ status: 'restored', config_imported: [...keys], db_restored: true/false }`
8. On any error: return 400 with descriptive message

**ZIP List -- GET /api/v1/backup/full/list:**
1. List all `.zip` files in `backup_dir`
2. For each: return `{ filename, size_bytes, created_at }` (parse timestamp from filename)
3. Sort by created_at descending

**Statistics -- GET /api/v1/statistics:**
1. Accept query param `range` (7d, 30d, 90d, 365d) defaulting to 30d
2. Parse to integer days: `{"7d": 7, "30d": 30, "90d": 90, "365d": 365}`
3. Query `daily_stats` with `ORDER BY date DESC LIMIT ?` using the days value
4. Query `provider_stats` using existing `get_provider_stats()` (no filter = all providers)
5. Query `subtitle_downloads` grouped by `provider_name` with COUNT and AVG(score)
6. Query `translation_backend_stats` for backend usage (all rows)
7. Query `upgrade_history` grouped by `old_format || ' -> ' || new_format` with COUNT
8. Return JSON with: `daily` (array of date rows), `providers` (provider stats dict), `downloads_by_provider` (array), `backend_stats` (array), `upgrades` (array), `range` (echo param)

**Statistics Export -- GET /api/v1/statistics/export:**
1. Accept query params: `range` (same as above), `format` ('json' or 'csv', default 'json')
2. Fetch same data as statistics endpoint
3. If format=json: return full statistics JSON with `Content-Disposition: attachment; filename=sublarr_stats_<date>.json`
4. If format=csv: build CSV using `csv.writer` and `io.StringIO`, include daily stats rows with headers (date, translated, failed, skipped). Return with `Content-Disposition: attachment; filename=sublarr_stats_<date>.csv`, mimetype `text/csv`

**Log Download -- GET /api/v1/logs/download:**
1. Read `log_file` path from settings
2. Return `send_file(log_file, mimetype='text/plain', as_attachment=True, download_name='sublarr.log')`
3. Return 404 if file doesn't exist

**Log Rotation Config -- PUT /api/v1/logs/rotation:**
1. Accept JSON body: `{ max_size_mb: number, backup_count: number }`
2. Validate max_size_mb between 1 and 100, backup_count between 1 and 20
3. Save as config_entries: `log_max_size_mb`, `log_backup_count`
4. Return `{ status: 'updated', max_size_mb, backup_count }`
5. NOTE: Actually applying rotation requires RotatingFileHandler reconfiguration which happens on next restart -- document this in the response.

**GET /api/v1/logs/rotation:**
1. Read `log_max_size_mb` and `log_backup_count` from config_entries (defaults: 10, 5)
2. Return `{ max_size_mb, backup_count }`
  </action>
  <verify>
1. `cd backend && python -c "from routes.system import bp; print('OK')"` -- imports without error
2. `cd backend && python -m pytest tests/ -x -q 2>&1 | tail -5` -- no new test failures
3. Start dev server: `npm run dev:backend`, then:
   - `curl -X POST http://localhost:5765/api/v1/backup/full` returns 201 with ZIP info
   - `curl http://localhost:5765/api/v1/backup/full/list` returns list with the new ZIP
   - `curl http://localhost:5765/api/v1/statistics?range=7d` returns daily + provider stats
   - `curl http://localhost:5765/api/v1/logs/download -o /dev/null -w "%{http_code}"` returns 200
  </verify>
  <done>
- ZIP backup creates valid archive with manifest + config + DB
- ZIP restore validates, imports config, restores DB
- Statistics endpoint returns comprehensive data with time range filter
- Statistics export returns JSON or CSV attachment
- Log download returns log file as attachment
- Log rotation config readable and writable
  </done>
</task>

<task type="auto">
  <name>Task 2: Subtitle processing tools Blueprint</name>
  <files>
    backend/routes/tools.py
    backend/app.py
  </files>
  <action>
**Create `backend/routes/tools.py`:**
New Blueprint with `url_prefix="/api/v1/tools"`. Import from existing `hi_remover` and `ass_utils` modules.

**POST /tools/remove-hi:**
1. Accept JSON: `{ file_path: string }`
2. Validate file_path exists and ends with `.srt` or `.ass`
3. Validate file_path is under the configured `media_path` (security: no arbitrary file access)
4. Read file content
5. Call `remove_hi_markers(content)` from `hi_remover.py`
6. Write cleaned content back to the same file (create backup first: copy original to `<name>.bak.<ext>`)
7. Return `{ status: 'cleaned', original_lines: N, cleaned_lines: M, removed: N-M }`

**POST /tools/adjust-timing:**
1. Accept JSON: `{ file_path: string, offset_ms: number }` (positive = delay, negative = advance)
2. Validate file_path exists and is under media_path
3. For SRT files: parse timestamps with regex `(\d{2}):(\d{2}):(\d{2}),(\d{3})`, add offset_ms to each timestamp, handle underflow (clamp to 00:00:00,000)
4. For ASS files: parse `Dialogue:` lines, timestamps are `H:MM:SS.cc` format (centiseconds), add offset converting ms to cs
5. Create backup before modifying
6. Return `{ status: 'adjusted', lines_modified: N, offset_ms: offset_ms }`

**POST /tools/common-fixes:**
1. Accept JSON: `{ file_path: string, fixes: string[] }` where fixes is array of: `['encoding', 'whitespace', 'linebreaks', 'empty_lines']`
2. Validate file_path
3. Apply selected fixes:
   - `encoding`: Read with chardet detection (if available, else utf-8), re-encode as UTF-8
   - `whitespace`: Strip trailing whitespace from each line
   - `linebreaks`: Normalize line endings to `\n` (remove `\r`)
   - `empty_lines`: Remove consecutive empty lines (keep single blank lines)
4. For ASS files, additionally call `fix_line_breaks()` from `ass_utils.py` if `linebreaks` is in fixes
5. Create backup before modifying
6. Return `{ status: 'fixed', fixes_applied: [...], lines_before: N, lines_after: M }`

**GET /tools/preview:**
1. Accept query param `file_path`
2. Validate file is under media_path, exists, ends with .srt or .ass
3. Read first 100 lines of the file
4. Return `{ format: 'srt'|'ass', lines: [...], total_lines: N, encoding: detected_encoding }`

**Register Blueprint in app.py:**
Add `from routes.tools import bp as tools_bp` and `app.register_blueprint(tools_bp)` in the `create_app()` function alongside the other blueprint registrations.
  </action>
  <verify>
1. `cd backend && python -c "from routes.tools import bp; print('OK')"` -- imports without error
2. `cd backend && python -c "from app import create_app; app = create_app(); print([r.rule for r in app.url_map.iter_rules() if 'tools' in r.rule])"` -- shows tools routes
3. `cd backend && python -m pytest tests/ -x -q 2>&1 | tail -5` -- no new test failures
  </verify>
  <done>
- Subtitle tools Blueprint registered with 4 endpoints
- HI removal wraps existing hi_remover.py
- Timing adjustment handles both SRT and ASS timestamp formats
- Common fixes handle encoding, whitespace, linebreaks, empty lines
- All tools validate file_path is under media_path (security)
- All tools create .bak backup before modifying files
- Preview endpoint returns file contents for UI display
  </done>
</task>

</tasks>

<verification>
1. All new endpoints return correct HTTP status codes (201, 200, 400, 404)
2. ZIP backup creates valid archive that can be extracted
3. ZIP restore validates manifest before proceeding
4. Statistics endpoint respects time range parameter
5. Tools endpoints validate file paths against media_path
6. No existing tests broken
7. Flask app starts with new tools Blueprint registered
</verification>

<success_criteria>
- BKUP-01 (Backup System): ZIP creation with config + DB -- COMPLETE
- BKUP-02 (Restore System): ZIP upload, validation, merge -- COMPLETE
- ADMN-01 backend: Statistics API with time range and export -- COMPLETE
- ADMN-02 backend: Log download and rotation config -- COMPLETE
- ADMN-04 backend: Subtitle tools API endpoints -- COMPLETE
</success_criteria>

<output>
After completion, create `.planning/phases/08-i18n-backup-admin-polish/08-02-SUMMARY.md`
</output>
