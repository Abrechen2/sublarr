---
phase: 21-translation-quality-scoring
plan: 01
type: execute
wave: 1
depends_on: ["20-02"]
files_modified:
  - backend/translator.py
  - backend/translation/__init__.py
  - backend/db/models/core.py
  - backend/db/repositories/jobs.py
  - backend/routes/tools.py
autonomous: true

must_haves:
  truths:
    - "After translating a line, the LLM evaluates its own output and assigns a quality score (0-100)"
    - "Lines below a configurable threshold are automatically retried (up to configurable max retries)"
    - "Quality scores are stored per translated file and aggregate in job stats"
    - "Config: quality threshold and max retries in config_entries"
  artifacts:
    - path: "backend/translator.py"
      provides: "Quality score collection and retry logic; job stats include avg_quality, low_quality_count"
    - path: "backend/translation/"
      provides: "Evaluation prompt and score parsing in backend or shared LLM helper"
    - path: "backend/db/repositories/jobs.py"
      provides: "stats_json includes quality_score / avg_quality / low_quality_lines"
    - path: "backend/routes/tools.py"
      provides: "Optional: /parse or new endpoint returns per-cue quality_score when available"
  key_links:
    - from: "backend/translator.py"
      to: "backend/translation"
      via: "evaluation step after translate_batch"
    - from: "backend/translator.py"
      to: "backend/db/repositories/jobs.py"
      via: "update_job with result.stats including quality"
---

<objective>
Backend: LLM self-evaluation of translation quality per line; retry low-quality lines; store scores in job stats and per-line for editor.

Purpose: Every translated line gets a quality score (0-100); lines below threshold are retried up to max_retries; scores are persisted so the editor and job history can show them.

Output: Evaluation prompt/step after translation; retry loop for low-scoring lines; config for threshold and max_retries; job stats_json extended with quality aggregates; per-line scores available for API (e.g. /tools/parse or sidecar).
</objective>

<execution_context>
@C:/Users/Dennis Wittke/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Dennis Wittke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/REQUIREMENTS.md

@backend/translator.py
@backend/translation/__init__.py
@backend/db/models/core.py
@backend/db/repositories/jobs.py
@backend/routes/tools.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Evaluation step and retry logic</name>
  <files>
    backend/translator.py
    backend/translation/__init__.py
  </files>
  <action>
1. After each batch (or per-line) translation, call an evaluation step: send (source_line, translated_line) to LLM with a short evaluation prompt asking for a score 0-100 and optional one-line reason. Parse score from response.
2. Config: translation_quality_threshold (default 50), translation_quality_max_retries (default 2). From config_entries.
3. For each line scoring below threshold, retry translation (same or next backend) up to max_retries; keep best score. If still below after retries, keep the best translation and score.
4. Collect per-line scores in order; compute aggregate: avg_quality, min_quality, count_below_threshold for job stats.
  </action>
  <verify>
    Unit test or manual: mock evaluation returning low score; confirm retry is triggered. Mock high score; confirm no retry.
  </verify>
  <done>
    Evaluation prompt and parsing implemented; retry loop for low scores; config for threshold and max_retries.
  </done>
</task>

<task type="auto">
  <name>Task 2: Job stats and per-line score storage</name>
  <files>
    backend/translator.py
    backend/db/repositories/jobs.py
  </files>
  <action>
1. When updating job on translation completion, include in result.stats: avg_quality (float), low_quality_lines (int), min_quality (int). Persist via update_job(..., result=...) so stats_json contains these.
2. Per-line scores: either (a) store in a sidecar file (e.g. .quality.json next to subtitle) or (b) extend a table that maps file_path + line_index to score. Option (b) requires a new table (e.g. translation_line_scores: job_id or file_path, line_index, score). Choose one approach and document. Editor needs scores via /tools/parse or GET content+metadata â€” ensure backend can return quality_score per cue when available.
  </action>
  <verify>
    Complete a translation job; inspect job row stats_json for avg_quality, low_quality_lines. Confirm per-line scores available for editor endpoint.
  </verify>
  <done>
    Job stats include quality aggregates; per-line scores stored and retrievable for editor.
  </done>
</task>

<task type="auto">
  <name>Task 3: Expose per-line scores to editor API</name>
  <files>
    backend/routes/tools.py
  </files>
  <action>
1. Extend POST /tools/parse response (or add optional query) so each cue can include quality_score when the file has associated scores (e.g. from last translation job or sidecar). If using a table, join or lookup by file_path; return scores in same order as cues.
2. Alternatively: GET /api/v1/jobs/:id/quality-scores returning { "scores": [ ... ] } for that job's file. Frontend can merge by job when viewing that file. Prefer integrating into /parse for simplicity if possible.
  </action>
  <verify>
    Call /parse for a file that was just translated; response includes quality_score per cue (or separate array).
  </verify>
  <done>
    Editor API returns per-line quality scores when available.
  </done>
</task>

</tasks>

<verification>
1. Evaluation runs after translation; scores parsed; retry for low scores up to max_retries.
2. Job stats_json includes avg_quality, low_quality_lines (and optionally min_quality).
3. Per-line scores stored and exposed via /parse or dedicated endpoint.
4. Config: translation_quality_threshold, translation_quality_max_retries.
</verification>

<success_criteria>
- Each translated line receives an LLM quality score (0-100).
- Low-scoring lines are retried up to max_retries.
- Job history shows aggregate quality; editor can show per-line scores.
</success_criteria>

<output>
After completion, create `.planning/phases/21-translation-quality-scoring/21-01-SUMMARY.md`
</output>
