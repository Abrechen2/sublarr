---
phase: 01-provider-plugin-expansion
plan: 06
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - backend/providers/titrari.py
  - backend/providers/legendasdivx.py
autonomous: true

must_haves:
  truths:
    - "User can search and download Romanian subtitles from Titrari"
    - "User can search and download Brazilian/Portuguese subtitles from LegendasDivx"
    - "Both providers handle HTML scraping defensively (parse failures logged, never crash)"
    - "LegendasDivx session cookies persist and re-authenticate on expiry"
    - "LegendasDivx daily search limit resets at midnight and never exceeds 140 searches per day"
    - "Both providers extract subtitles from RAR/ZIP archives"
  artifacts:
    - path: "backend/providers/titrari.py"
      provides: "Titrari HTML scraping provider for Romanian subs"
      contains: "@register_provider"
      exports: ["TitrariProvider"]
    - path: "backend/providers/legendasdivx.py"
      provides: "LegendasDivx HTML scraping provider for Portuguese subs"
      contains: "@register_provider"
      exports: ["LegendasDivxProvider"]
  key_links:
    - from: "backend/providers/titrari.py"
      to: "backend/providers/base.py"
      via: "extends SubtitleProvider"
      pattern: "class TitrariProvider.SubtitleProvider"
    - from: "backend/providers/legendasdivx.py"
      to: "backend/providers/base.py"
      via: "extends SubtitleProvider"
      pattern: "class LegendasDivxProvider.SubtitleProvider"
    - from: "backend/providers/legendasdivx.py"
      to: "backend/providers/http_session.py"
      via: "session with cookie persistence for auth"
      pattern: "create_session"
---

<objective>
Implement two high-complexity HTML scraping providers: Titrari (Romanian subtitles, PROV-07) and LegendasDivx (Brazilian/Portuguese subtitles, PROV-08). Both require BeautifulSoup-based HTML parsing, RAR/ZIP archive extraction, and careful error handling for fragile scraping targets.

Purpose: These providers cover Romanian and Portuguese/Brazilian subtitle markets that have no API-based alternatives. They complete the 8-provider expansion goal for Phase 1.

Output: Two new built-in providers, completing the full set of 8 new providers for this phase.
</objective>

<execution_context>
@C:/Users/Dennis Wittke/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/Dennis Wittke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-provider-plugin-expansion/01-RESEARCH.md

# Plan 01 creates the declarative config_fields pattern this plan uses
@.planning/phases/01-provider-plugin-expansion/01-01-SUMMARY.md

@backend/providers/base.py
@backend/providers/__init__.py
@backend/providers/http_session.py
@backend/providers/animetosho.py
@backend/providers/jimaku.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement Titrari provider (Romanian subtitle scraping)</name>
  <files>backend/providers/titrari.py</files>
  <action>
    Create `backend/providers/titrari.py` implementing HTML scraping against titrari.ro:

    Site: `https://www.titrari.ro`

    Class: `TitrariProvider(SubtitleProvider)` with `@register_provider` decorator.
    - `name = "titrari"`
    - `languages = {"ro"}` -- Romanian only
    - `config_fields = []` -- no auth required (uses User-Agent spoofing like a browser)
    - `rate_limit = (10, 60)` -- polite scraping: 10 requests per minute
    - `timeout = 20`
    - `max_retries = 2`

    **initialize():**
    - Create session via `create_session()` with a browser-like User-Agent header:
      `Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36`
    - Set `Accept-Language: ro-RO,ro;q=0.9,en;q=0.8` header.

    **search(query) implementation:**
    1. Only search for Romanian. If "ro" not in `query.languages`, return [].
    2. Determine search term: for episodes use `{series_title} S{season:02d}E{episode:02d}`, for movies use `{title}` (append year if available).
    3. Build search URL: `GET https://www.titrari.ro/index.php?page=cautare&titlufilm={search_term}`.
    4. Parse HTML response with BeautifulSoup (use lxml parser if available, fall back to html.parser).
    5. Find the results table. Titrari renders results in an HTML table with rows containing:
       - Subtitle title/filename
       - Language indicator
       - Format info (SRT/SUB)
       - Download link
    6. For each table row that looks like a subtitle result:
       - Extract the download link (typically href to a download page or direct .zip/.rar link).
       - Extract release name / filename from the text content.
       - Detect format from filename extension or default to SRT.
       - Use `guessit` (if available) to parse release name for series/episode matching. Fall back to regex patterns if guessit is not installed.
       - Build `SubtitleResult` with `matches` based on what was detected.
    7. Handle parse failures defensively: wrap each row parse in try/except, log warning, continue to next row.
    8. Handle empty results page (no table or empty table) gracefully -- return [].

    **download(result) implementation:**
    1. If download_url points to a download page (not direct file), fetch the page and extract the actual download link.
    2. `GET {download_url}` to fetch the archive.
    3. Detect archive type from content:
       - ZIP: `content[:4] == b'PK\x03\x04'` -- extract using `zipfile`.
       - RAR: `content[:4] == b'Rar!'` -- extract using `rarfile` (already in requirements).
       - Otherwise treat content as raw subtitle file.
    4. Extract the first subtitle file (.srt, .ass, .sub) from the archive.
    5. Return content bytes.

    **health_check():**
    - `GET https://www.titrari.ro/` -- check for 200 status and presence of expected HTML structure (search form).

    **Error handling:**
    - Import BeautifulSoup conditionally with graceful fallback (log warning, return [] from search).
    - Import guessit conditionally with regex fallback for episode parsing.
    - Import rarfile conditionally -- if not available, log warning when RAR archive encountered.
    - All parse errors logged as warnings, never crash the provider.
    - Handle HTTP 403 (blocked) by raising ProviderError with clear message.
  </action>
  <verify>
    - `cd backend && python -c "from providers.titrari import TitrariProvider; p = TitrariProvider(); assert p.name == 'titrari'; assert 'ro' in p.languages; print('Titrari OK')"`
    - `cd backend && python -c "from providers import _PROVIDER_CLASSES; assert 'titrari' in _PROVIDER_CLASSES; print('Titrari registered')"`
    - `cd backend && python -m pytest tests/ -x -q --timeout=30`
  </verify>
  <done>
    TitrariProvider is registered, scrapes titrari.ro for Romanian subtitles, parses HTML tables with BeautifulSoup, extracts subtitles from RAR/ZIP archives, and handles all parsing failures defensively.
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement LegendasDivx provider (Portuguese subtitle scraping with session auth and daily limit)</name>
  <files>backend/providers/legendasdivx.py</files>
  <action>
    Create `backend/providers/legendasdivx.py` implementing HTML scraping against legendasdivx.pt:

    Site: `https://www.legendasdivx.pt`

    Class: `LegendasDivxProvider(SubtitleProvider)` with `@register_provider` decorator.
    - `name = "legendasdivx"`
    - `languages = {"pt"}` -- Portuguese (including Brazilian Portuguese)
    - `config_fields = [{"key": "username", "label": "Username", "type": "text", "required": True}, {"key": "password", "label": "Password", "type": "password", "required": True}]`
    - `rate_limit = (5, 60)` -- very conservative due to 145 searches/day limit
    - `timeout = 20`
    - `max_retries = 1` -- do not waste rate limit on retries

    **initialize():**
    - Create session via `create_session()` with browser-like User-Agent.
    - Do NOT login during initialize -- defer login to first search (lazy auth).
    - Store session cookies for persistence across searches.
    - Initialize daily limit tracking:
      ```python
      self._search_count = 0
      self._last_reset_date = date.today()
      ```

    **_login() internal method:**
    1. `GET https://www.legendasdivx.pt/forum/ucp.php?mode=login` to get the login form and any CSRF tokens.
    2. Parse the form with BeautifulSoup to extract hidden fields (sid, form token, etc.).
    3. `POST` the login form with username, password, and hidden fields.
    4. Check response: if redirected to index or contains "logout" link, login succeeded. Otherwise, raise ProviderAuthError.
    5. Store the session cookies -- they will be reused for subsequent requests.

    **_ensure_authenticated() internal method:**
    1. First, check daily limit reset: compare `date.today()` against `self._last_reset_date`. If `date.today() > self._last_reset_date`, reset the counter:
       ```python
       from datetime import date
       today = date.today()
       if today > self._last_reset_date:
           self._search_count = 0
           self._last_reset_date = today
           logger.info("LegendasDivx: daily search counter reset (new day)")
       ```
    2. Check daily limit: if `self._search_count >= 140` (safety margin below 145 limit), raise ProviderRateLimitError with message "Daily search limit reached (140/145). Resets at midnight."
    3. Check if session cookies are still valid by looking for a logged-in indicator.
    4. If cookies expired (detected via 302 redirect to login page), call `_login()`.

    **search(query) implementation:**
    1. Only search for Portuguese. If "pt" not in `query.languages`, return [].
    2. Call `_ensure_authenticated()`.
    3. Build search term: for episodes `{series_title} S{season:02d}E{episode:02d}`, for movies `{title} {year}`.
    4. `POST https://www.legendasdivx.pt/modules.php?name=Downloads&d_op=search` with form data:
       - `pesession`: search term
       - `selession`: subtitle type filter (empty for all)
    5. Increment `self._search_count`.
    6. Parse HTML response with BeautifulSoup:
       - Results are displayed in a list/table format with subtitle entries.
       - Each entry contains: title, release info, language flag, download link, format info.
    7. For each subtitle entry:
       - Extract download page link (typically links to a detail page).
       - Extract release name, language, and format from the entry HTML.
       - Use guessit (if available) for release name parsing, with regex fallback.
       - Build `SubtitleResult` with appropriate `matches`.
    8. Handle parse failures defensively per entry.

    **download(result) implementation:**
    1. Call `_ensure_authenticated()`.
    2. If download_url points to a detail page, fetch it and extract the actual download link.
    3. `GET {download_url}` to download the archive.
    4. Detect and extract from RAR or ZIP archive (same logic as Titrari):
       - ZIP: use zipfile module.
       - RAR: use rarfile module.
    5. Extract first subtitle file (.srt, .ass).
    6. Return content bytes.

    **health_check():**
    - Try to access the site and check if login form is present (site is up).
    - Do NOT actually login during health check (wastes session).

    **terminate():**
    - Close session and clear cookies.
    - Reset `_search_count` and `_last_reset_date`.

    **Critical considerations from research:**
    - 145 searches/day limit is strict. Track daily count with `_search_count`, reset by comparing `date.today()` against `_last_reset_date` at the start of each `_ensure_authenticated()` call.
    - PHP session cookies have short TTLs. Detect session expiry via 302 redirects.
    - Do not retry on auth failures to avoid burning rate limit.
    - Import BeautifulSoup, guessit, and rarfile conditionally with graceful fallback.
    - All parse errors logged as warnings.
  </action>
  <verify>
    - `cd backend && python -c "from providers.legendasdivx import LegendasDivxProvider; p = LegendasDivxProvider(); assert p.name == 'legendasdivx'; assert 'pt' in p.languages; assert len(p.config_fields) == 2; print('LegendasDivx OK')"`
    - `cd backend && python -c "from providers import _PROVIDER_CLASSES; assert 'legendasdivx' in _PROVIDER_CLASSES; print('LegendasDivx registered')"`
    - `cd backend && python -m pytest tests/ -x -q --timeout=30`
  </verify>
  <done>
    LegendasDivxProvider is registered, handles session-based auth with lazy login, scrapes Portuguese subtitles with daily rate limit tracking (140/145 safety margin with midnight reset via date comparison), extracts from RAR/ZIP archives, and handles session expiry with re-authentication.
  </done>
</task>

</tasks>

<verification>
1. Both providers registered: `cd backend && python -c "from providers import _PROVIDER_CLASSES; assert 'titrari' in _PROVIDER_CLASSES and 'legendasdivx' in _PROVIDER_CLASSES; print('Both registered:', sorted(_PROVIDER_CLASSES.keys()))"`
2. All existing tests pass: `cd backend && python -m pytest tests/ -x -q --timeout=30`
3. Full provider count: `cd backend && python -c "from providers import _PROVIDER_CLASSES; print(f'{len(_PROVIDER_CLASSES)} providers total:', sorted(_PROVIDER_CLASSES.keys()))"` -- should show 12 providers (4 original + 8 new).
</verification>

<success_criteria>
- TitrariProvider: scrapes titrari.ro for Romanian subs, parses HTML tables, extracts from RAR/ZIP
- LegendasDivxProvider: session-based auth with lazy login, scrapes legendasdivx.pt for Portuguese subs
- LegendasDivx tracks daily search count with safety margin below 145/day limit
- LegendasDivx daily counter resets when date.today() > _last_reset_date (midnight boundary)
- LegendasDivx detects session expiry (302 redirect) and re-authenticates
- Both use @register_provider and declare config_fields
- Both import BeautifulSoup/guessit/rarfile conditionally with graceful fallback
- All parse errors logged as warnings, never crash
- All existing tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/01-provider-plugin-expansion/01-06-SUMMARY.md`
</output>
